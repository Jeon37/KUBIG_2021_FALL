{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"BertShared.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1d3Hm_DimsN8j1UL_SNb2ciDKxLUvarHE","authorship_tag":"ABX9TyMBOA4FhL9EgRKR3RRtstCI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"60wnq3EgjC_k"},"source":["# 데이터 가져오기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TFWA7Hlbgahn","executionInfo":{"status":"ok","timestamp":1633259027028,"user_tz":-540,"elapsed":826,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"a34cbe3c-980d-4ce1-ee2c-ba2a3e43c549"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"poQvCXdngbo4","executionInfo":{"status":"ok","timestamp":1633251926708,"user_tz":-540,"elapsed":6,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"90155573-7872-49bc-c3b2-6ea666deea01"},"source":["cd /content/drive/Shareddrives/KLUE Summarization/"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shareddrives/KLUE Summarization\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UgLecFERhghT","executionInfo":{"status":"ok","timestamp":1633251927681,"user_tz":-540,"elapsed":976,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"219774a4-17df-418d-b79d-944b76b824cc"},"source":["!ls"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":[" 모델링\t\t   'klue crawling'\t'문서요약 텍스트_unzip'\n","'도서자료 요약'     models\n","'문서요약 텍스트'  'Preprocessed Data'\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"h74yYX5tl_el","executionInfo":{"status":"ok","timestamp":1633251945986,"user_tz":-540,"elapsed":18308,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"081f2d33-be99-4ea7-9c44-6c0f61acc10d"},"source":["import json\n","import pandas as pd\n","\n","def read_jsonl(path):\n","  DATA_DIR = \"문서요약 텍스트_unzip\"\n","  with open(DATA_DIR + path , 'r') as json_file:\n","      json_list = list(json_file)\n","\n","  trains = []\n","\n","  for json_str in json_list:\n","      line = json.loads(json_str)\n","      trains.append(line)\n","  df = pd.DataFrame(trains)\n","  return df\n","\n","train_df = read_jsonl('/1.Training/신문기사_1.train.jsonl/train.jsonl')\n","val_df = read_jsonl('/2.Validation/신문기사_1.vaild.jsonl/vaild.jsonl')\n","train_df.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>media</th>\n","      <th>id</th>\n","      <th>article_original</th>\n","      <th>abstractive</th>\n","      <th>extractive</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>부산일보</td>\n","      <td>360972161</td>\n","      <td>[지난해 고령화와 유례가 드문 겨울 한파 등 영향으로 우리나라 사망자 수가 통계 작...</td>\n","      <td>통계청이 발표한 '2018년 사망원인통계'를 보면 지난해 총 사망자 수는 관련 통계...</td>\n","      <td>[4, 11, 18]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>중도일보</td>\n","      <td>356659913</td>\n","      <td>[서산시의회(의장 임재관) 가충순·이수의 의원이 (사)한국지역신문협회에서 수여하는 ...</td>\n","      <td>서산시 가충순 의원과 이수의 의원이 활발한 의정활동을 펼친 감사의 표시로 한국지역신...</td>\n","      <td>[1, 3, 4]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>무등일보</td>\n","      <td>351718460</td>\n","      <td>[지난 2004년 시작해 조선대 학생들의 대표적인 행사로 자리매김한 ‘조선대 국토대...</td>\n","      <td>‘조선대의 새로운 비상을 꿈꾸다’를 슬로건으로 진행되어 단체생활을 통해 협동심과 ...</td>\n","      <td>[0, 2, 4]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>이데일리</td>\n","      <td>335868123</td>\n","      <td>[서울시는 신학기가 시작되는 다음달 4일부터 고등학교 3학년 무상급식을 실시한다고 ...</td>\n","      <td>서울시가 다음달 4일부터 서울 시내 319개 고등학교 3학년 8만4700명을 대상으...</td>\n","      <td>[0, 1, 2]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>서울신문</td>\n","      <td>351443347</td>\n","      <td>[미국인 선교사가 우간다에서 의사 행세를 하며 의료 시설을 운영한 혐의로 지역 시민...</td>\n","      <td>미국인 선교사가 우간다에서 의사 행세를 하며 두 아이의 죽음과 관련돼 있다며 지역 ...</td>\n","      <td>[0, 1, 2]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  media  ...   extractive\n","0  부산일보  ...  [4, 11, 18]\n","1  중도일보  ...    [1, 3, 4]\n","2  무등일보  ...    [0, 2, 4]\n","3  이데일리  ...    [0, 1, 2]\n","4  서울신문  ...    [0, 1, 2]\n","\n","[5 rows x 5 columns]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uDa2Q3DetJE-","executionInfo":{"status":"ok","timestamp":1633251945986,"user_tz":-540,"elapsed":8,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"9556d4e7-c846-4ebb-8517-7819da8899b1"},"source":["# join으로 str로 바꿔줘야할듯\n","for i,info in enumerate(train_df['article_original'][1]):\n","  print('line {} : {}'.format(i,info))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["line 0 : 서산시의회(의장 임재관) 가충순·이수의 의원이 (사)한국지역신문협회에서 수여하는 우수의정대상을 받았다.\n","line 1 : 가충순 의원과 이수의 의원은 16일 팔봉면 폰타나 리조트에서 열린 한국지역신문협회 하계 워크샵에서 지역사회 발전을 위해 활발한 의정활동을 펼친 공로를 인정받아 우수의정대상을 수상했다.\n","line 2 : 지난해 6월 제7회 전국동시지방선거를 통해 등원한 두 의원은 산업건설위원회에서 열정적인 의정활동을 펼치고 있다.\n","line 3 : 가충순 의원은 5분발언, 행정사무감사, 시정질문을 통해 자동차 연비테스트 연구시설 유치, 천수만 염해피해 재발 방지, 서산시 대표 농산물 육성 등 지역의 크고 작은 문제를 개선하기 위해 노력하고 있다.\n","line 4 : 이수의 의원은 지난 행정사무감사에서 대산공단 기업 임원을 참고인으로 출석시켜 지역인재채용 및 관내업체·자재 활용을 확대할 것을 제안하며 기존 행정사무감사의 틀을 깨는 등 다양한 의정활동을 펼쳐나가고 있다.\n","line 5 : 가충순 의원은 \"시의원이라면 마땅히 해야할 일을 한 것 뿐인데 상까지 주시니 몸 둘 바를 모르겠다\"며 \"항상 초심을 잊지 않고 지역 발전을 위해 최선을 다하겠다\"고 소감을 밝혔다.\n","line 6 : 이수의 의원은 \"믿고 뽑아주신 주민들을 위해 당연히 해야할 일을 했을 뿐인데 상까지 받게 돼 영광\"이라며 \"시민들이 자부심을 느낄 수 있는 지역사회를 만들어 나가기 위해 앞으로도 최선을 다 하겠다\"고 말했다.\n"]}]},{"cell_type":"code","metadata":{"id":"VC7j9QLd3A6_","executionInfo":{"status":"ok","timestamp":1633251945986,"user_tz":-540,"elapsed":5,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}}},"source":["def articles_to_str(articles_list):\n","  return ' '.join(articles_list)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUmTSjh13Un9","executionInfo":{"status":"ok","timestamp":1633251946461,"user_tz":-540,"elapsed":479,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}}},"source":["train_df['article_to_str']=train_df['article_original'].apply(articles_to_str)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535},"id":"5VZu0u_Ea0g2","executionInfo":{"status":"ok","timestamp":1633252238063,"user_tz":-540,"elapsed":398,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"3f5d964f-f3bf-40b9-9863-c7b86e028647"},"source":["train_df.head()"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>media</th>\n","      <th>id</th>\n","      <th>article_original</th>\n","      <th>abstractive</th>\n","      <th>extractive</th>\n","      <th>article_to_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>부산일보</td>\n","      <td>360972161</td>\n","      <td>[지난해 고령화와 유례가 드문 겨울 한파 등 영향으로 우리나라 사망자 수가 통계 작...</td>\n","      <td>통계청이 발표한 '2018년 사망원인통계'를 보면 지난해 총 사망자 수는 관련 통계...</td>\n","      <td>[4, 11, 18]</td>\n","      <td>지난해 고령화와 유례가 드문 겨울 한파 등 영향으로 우리나라 사망자 수가 통계 작성...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>중도일보</td>\n","      <td>356659913</td>\n","      <td>[서산시의회(의장 임재관) 가충순·이수의 의원이 (사)한국지역신문협회에서 수여하는 ...</td>\n","      <td>서산시 가충순 의원과 이수의 의원이 활발한 의정활동을 펼친 감사의 표시로 한국지역신...</td>\n","      <td>[1, 3, 4]</td>\n","      <td>서산시의회(의장 임재관) 가충순·이수의 의원이 (사)한국지역신문협회에서 수여하는 우...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>무등일보</td>\n","      <td>351718460</td>\n","      <td>[지난 2004년 시작해 조선대 학생들의 대표적인 행사로 자리매김한 ‘조선대 국토대...</td>\n","      <td>‘조선대의 새로운 비상을 꿈꾸다’를 슬로건으로 진행되어 단체생활을 통해 협동심과 ...</td>\n","      <td>[0, 2, 4]</td>\n","      <td>지난 2004년 시작해 조선대 학생들의 대표적인 행사로 자리매김한 ‘조선대 국토대장...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>이데일리</td>\n","      <td>335868123</td>\n","      <td>[서울시는 신학기가 시작되는 다음달 4일부터 고등학교 3학년 무상급식을 실시한다고 ...</td>\n","      <td>서울시가 다음달 4일부터 서울 시내 319개 고등학교 3학년 8만4700명을 대상으...</td>\n","      <td>[0, 1, 2]</td>\n","      <td>서울시는 신학기가 시작되는 다음달 4일부터 고등학교 3학년 무상급식을 실시한다고 2...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>서울신문</td>\n","      <td>351443347</td>\n","      <td>[미국인 선교사가 우간다에서 의사 행세를 하며 의료 시설을 운영한 혐의로 지역 시민...</td>\n","      <td>미국인 선교사가 우간다에서 의사 행세를 하며 두 아이의 죽음과 관련돼 있다며 지역 ...</td>\n","      <td>[0, 1, 2]</td>\n","      <td>미국인 선교사가 우간다에서 의사 행세를 하며 의료 시설을 운영한 혐의로 지역 시민단...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  media  ...                                     article_to_str\n","0  부산일보  ...  지난해 고령화와 유례가 드문 겨울 한파 등 영향으로 우리나라 사망자 수가 통계 작성...\n","1  중도일보  ...  서산시의회(의장 임재관) 가충순·이수의 의원이 (사)한국지역신문협회에서 수여하는 우...\n","2  무등일보  ...  지난 2004년 시작해 조선대 학생들의 대표적인 행사로 자리매김한 ‘조선대 국토대장...\n","3  이데일리  ...  서울시는 신학기가 시작되는 다음달 4일부터 고등학교 3학년 무상급식을 실시한다고 2...\n","4  서울신문  ...  미국인 선교사가 우간다에서 의사 행세를 하며 의료 시설을 운영한 혐의로 지역 시민단...\n","\n","[5 rows x 6 columns]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70DWtE5B3xyK","executionInfo":{"status":"ok","timestamp":1633251946462,"user_tz":-540,"elapsed":6,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"08ce58b2-25f6-4c4b-a9ef-5dad61db7986"},"source":["print(train_df['article_to_str'][0])\n","print(type(train_df['article_to_str'][0]))\n"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["지난해 고령화와 유례가 드문 겨울 한파 등 영향으로 우리나라 사망자 수가 통계 작성 이후 가장 많았다. 폐렴과 치매의 일종인 알츠하이머병은 지난해 사망원인 순위 3위와 9위로 전년보다 각각 한 단계, 두 단계 상승하는 등 노인성 질병에 의한 사망률이 급증하는 추세다. ‘연령표준화 사망률’(표준인구 10만 명당 사망자 수)은 울산·충북·부산 순으로 높게 나타났다. ■작년 사망자 29만 8820명, 역대 최다 24일 통계청이 발표한 '2018년 사망원인통계'를 보면 지난해 총 사망자 수는 전년 대비 4.7%(1만 3286명) 증가한 29만 8820명으로 관련 통계를 작성한 1983년 이후 가장 많았으며, 5년 연속 증가세를 보였다. 통계청은 인구 구조의 고령화와 지난해 1~2월 유례가 드문 한파 등을 그 원인으로 꼽았다. 지난해 조사망률(인구 10만 명당 사망자 수) 역시 582.5명으로 전년보다 4.5%(25.1명) 증가해 5년 연속 늘었다. 특히 80세 이상의 사망자가 전체 사망자의 절반에 가까운 46.3%로, 10년 전보다 14.3%포인트(P)나 증가했다. ■폐렴·알츠하이머병 사망률 순위 ‘껑충’ 사망원인별로 보면 지난해 암(악성신생물)에 의한 사망률(이하 인구 10만 명당 사망자 수)은 154.3명으로 전년보다 0.2% 증가했다. 1983년 관련 통계를 집계한 이래 줄곧 암이 사망원인 1위로 집계됐다. 특히 폐렴(4위→3위)과 치매의 일종인 알츠하이머병(11위→9위)에 의한 사망률 순위 상승이 두드러졌다. 폐렴 사망률은 2004년 10위에서 꾸준히 순위가 상승하고 있고, 알츠하이머병 사망률 역시 통계 작성 이래 10대 사인에 처음 포함됐다. 지난해 알츠하이머병에 의한 사망률은 12.0명으로 전년(9.8명) 대비 22.5% 증가했다. 알츠하이머병 사망률은 10년 전(3.8명)과 비교하면 무려 214.2% 증가했다. 폐렴 사망률은 45.4명으로 전년(37.8명) 대비 20.0% 증가했다. 알코올 관련 사망률은 9.6명으로 전년보다 2.0% 늘었다. ■자살률 5년 만에 증가…\"베르테르 효과 영향\" 지난해 자살에 의한 사망자는 1만 3670명으로 전년보다 9.7%(1207명) 증가했다. 자살률은 26.6명으로 전년보다 2.3명(9.5%) 증가했다. 자살률은 2013년 28.5명, 2014년 27.3명, 2015년 26.5명, 2016년 25.6명, 2017년 24.3명 등 4년 연속 줄어들다가 5년 만에 증가세로 돌아섰다. 자살은 10∼30대까지 사망원인 순위 1위를 차지했고, 40∼50대에서도 2위를 기록했다. 김진 통계청 인구동향과장은 \"자살에는 베르테르 효과, 즉 유명인 자살이 영향을 준다. 2011년 이후 유명인 자살이 줄면서 자살이 줄었는데 지난해에는 유명인 자살이 있어 영향을 줬다\"고 설명했다. 지역 간 연령구조 차이를 표준화한 사망률(표준인구 10만 명당 사망자 수)을 보면 울산(355.3명), 충북(352.6명), 부산(350.8명)이 높았고, 서울(283.3명)과 경기(306.8명)가 낮았다. 사인별로 연령표준화 사망률이 높은 지역을 보면 암은 경남(101.5명), 심장 질환은 경남(44.6명), 뇌혈관 질환은 울산(30.6명), 폐렴은 경북(30.3명), 운수사고는 전남(14.4명), 고의적 자해(자살)는 충남(29.8명)이었다.\n","<class 'str'>\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8UcKNK9ja3kv","executionInfo":{"status":"ok","timestamp":1633252327386,"user_tz":-540,"elapsed":521,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"c16877cb-2f3a-49b7-9e0c-72eeddb76001"},"source":["df=train_df['article_to_str']\n","print(len(train_df))\n","print(len(df))"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["260697\n","260697\n"]}]},{"cell_type":"markdown","metadata":{"id":"N9auBsPijGQz"},"source":["# Fine-Tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUXx1kvZLiaz","executionInfo":{"status":"ok","timestamp":1633251954878,"user_tz":-540,"elapsed":8420,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"a59d86f7-cef0-43b2-dce7-9240b90aa1e8"},"source":["!pip install transformers"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 12.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Collecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 42.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 38.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 34.5 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.2\n"]}]},{"cell_type":"code","metadata":{"id":"TCEu1YuFtgKK","executionInfo":{"status":"ok","timestamp":1633251960958,"user_tz":-540,"elapsed":6084,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}}},"source":["from transformers import BertTokenizerFast, EncoderDecoderModel"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"m9nA-A3Bh97F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633251994773,"user_tz":-540,"elapsed":2791,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"913e2eb1-ed60-4180-e14a-0e912cb1e700"},"source":["# BertTokenizerFast : huggingface의 transformers.PreTrainedTokenizerFast 상속\n","tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bertshared-kor-base\", \n","                                                   bos_token='<bos>', # begin of string [bos] token\n","                                                   eos_token='<eos>',  # end of string  [eos] token\n","                                                   pad_token='<pad>', # padding token\n","                                                   )"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-6314JBqom-I","executionInfo":{"status":"ok","timestamp":1633251994773,"user_tz":-540,"elapsed":21,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"eba084bb-cff7-4ce7-ce7f-aab3c25519a0"},"source":["print('Input의 최대 토큰수 Transformer 모델 max length :',tokenizer.model_max_length)\n","print('BOS token : {}, BOS token id : {}'.format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id),tokenizer.bos_token_id))\n","print('EOS token : {}, BOS token id : {}'.format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id),tokenizer.eos_token_id))\n","print('padding token : {}, BOS token id : {}'.format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id),tokenizer.pad_token_id))\n"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Input의 최대 토큰수 Transformer 모델 max length : 512\n","BOS token : <bos>, BOS token id : 42000\n","EOS token : <eos>, BOS token id : 42001\n","padding token : <pad>, BOS token id : 42002\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rJMFCgcFqYvE","executionInfo":{"status":"ok","timestamp":1633251994774,"user_tz":-540,"elapsed":18,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"3a4fe04d-8e82-41f7-f6c0-ba94729f4ab8"},"source":["ex=train_df['article_original'][0]\n","ex=' '.join(ex)\n","print(ex)\n","print(type(ex))"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["지난해 고령화와 유례가 드문 겨울 한파 등 영향으로 우리나라 사망자 수가 통계 작성 이후 가장 많았다. 폐렴과 치매의 일종인 알츠하이머병은 지난해 사망원인 순위 3위와 9위로 전년보다 각각 한 단계, 두 단계 상승하는 등 노인성 질병에 의한 사망률이 급증하는 추세다. ‘연령표준화 사망률’(표준인구 10만 명당 사망자 수)은 울산·충북·부산 순으로 높게 나타났다. ■작년 사망자 29만 8820명, 역대 최다 24일 통계청이 발표한 '2018년 사망원인통계'를 보면 지난해 총 사망자 수는 전년 대비 4.7%(1만 3286명) 증가한 29만 8820명으로 관련 통계를 작성한 1983년 이후 가장 많았으며, 5년 연속 증가세를 보였다. 통계청은 인구 구조의 고령화와 지난해 1~2월 유례가 드문 한파 등을 그 원인으로 꼽았다. 지난해 조사망률(인구 10만 명당 사망자 수) 역시 582.5명으로 전년보다 4.5%(25.1명) 증가해 5년 연속 늘었다. 특히 80세 이상의 사망자가 전체 사망자의 절반에 가까운 46.3%로, 10년 전보다 14.3%포인트(P)나 증가했다. ■폐렴·알츠하이머병 사망률 순위 ‘껑충’ 사망원인별로 보면 지난해 암(악성신생물)에 의한 사망률(이하 인구 10만 명당 사망자 수)은 154.3명으로 전년보다 0.2% 증가했다. 1983년 관련 통계를 집계한 이래 줄곧 암이 사망원인 1위로 집계됐다. 특히 폐렴(4위→3위)과 치매의 일종인 알츠하이머병(11위→9위)에 의한 사망률 순위 상승이 두드러졌다. 폐렴 사망률은 2004년 10위에서 꾸준히 순위가 상승하고 있고, 알츠하이머병 사망률 역시 통계 작성 이래 10대 사인에 처음 포함됐다. 지난해 알츠하이머병에 의한 사망률은 12.0명으로 전년(9.8명) 대비 22.5% 증가했다. 알츠하이머병 사망률은 10년 전(3.8명)과 비교하면 무려 214.2% 증가했다. 폐렴 사망률은 45.4명으로 전년(37.8명) 대비 20.0% 증가했다. 알코올 관련 사망률은 9.6명으로 전년보다 2.0% 늘었다. ■자살률 5년 만에 증가…\"베르테르 효과 영향\" 지난해 자살에 의한 사망자는 1만 3670명으로 전년보다 9.7%(1207명) 증가했다. 자살률은 26.6명으로 전년보다 2.3명(9.5%) 증가했다. 자살률은 2013년 28.5명, 2014년 27.3명, 2015년 26.5명, 2016년 25.6명, 2017년 24.3명 등 4년 연속 줄어들다가 5년 만에 증가세로 돌아섰다. 자살은 10∼30대까지 사망원인 순위 1위를 차지했고, 40∼50대에서도 2위를 기록했다. 김진 통계청 인구동향과장은 \"자살에는 베르테르 효과, 즉 유명인 자살이 영향을 준다. 2011년 이후 유명인 자살이 줄면서 자살이 줄었는데 지난해에는 유명인 자살이 있어 영향을 줬다\"고 설명했다. 지역 간 연령구조 차이를 표준화한 사망률(표준인구 10만 명당 사망자 수)을 보면 울산(355.3명), 충북(352.6명), 부산(350.8명)이 높았고, 서울(283.3명)과 경기(306.8명)가 낮았다. 사인별로 연령표준화 사망률이 높은 지역을 보면 암은 경남(101.5명), 심장 질환은 경남(44.6명), 뇌혈관 질환은 울산(30.6명), 폐렴은 경북(30.3명), 운수사고는 전남(14.4명), 고의적 자해(자살)는 충남(29.8명)이었다.\n","<class 'str'>\n"]}]},{"cell_type":"code","metadata":{"id":"zbROqwIZqFAd","executionInfo":{"status":"ok","timestamp":1633251994774,"user_tz":-540,"elapsed":15,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}}},"source":["outputs=tokenizer.encode('<bos>'+ex+'<eos>', \n","      truncation=True, max_length=512, padding=\"max_length\")[1:]"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FxxRAPANreEb","executionInfo":{"status":"ok","timestamp":1633253360593,"user_tz":-540,"elapsed":325,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"164a9242-c531-4f3c-d5ee-91cda93c0161"},"source":["for i in outputs[:]:\n","  print(tokenizer.convert_ids_to_tokens(i))"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["<bos>\n","지난해\n","고령\n","##화와\n","유\n","##례가\n","드문\n","겨울\n","한\n","##파\n","등\n","영향으로\n","우리나라\n","사망\n","##자\n","수가\n","통계\n","작성\n","이후\n","가장\n","많았다\n",".\n","폐\n","##렴\n","##과\n","치매\n","##의\n","일종\n","##인\n","알\n","##츠\n","##하이\n","##머\n","##병은\n","지난해\n","사망\n","##원인\n","순위\n","3위\n","##와\n","9\n","##위로\n","전년\n","##보다\n","각각\n","한\n","단계\n",",\n","두\n","단계\n","상승\n","##하는\n","등\n","노인\n","##성\n","질병\n","##에\n","의한\n","사망\n","##률이\n","급증\n","##하는\n","추세\n","##다\n",".\n","[UNK]\n","연령\n","##표준\n","##화\n","사망\n","##률\n","[UNK]\n","(\n","표준\n","##인구\n","10만\n","명\n","##당\n","사망\n","##자\n","수\n",")\n","은\n","울산\n","[UNK]\n","충북\n","[UNK]\n","부산\n","순으로\n","높게\n","나타났다\n",".\n","[UNK]\n","사망\n","##자\n","29\n","##만\n","88\n","##20\n","##명\n",",\n","역대\n","최다\n","24일\n","통계\n","##청이\n","발표한\n","[UNK]\n","2018년\n","사망\n","##원인\n","##통\n","##계\n","[UNK]\n","를\n","보면\n","지난해\n","총\n","사망\n","##자\n","수는\n","전년\n","대비\n","4\n",".\n","7\n","%\n","(\n","1만\n","32\n","##86\n","##명\n",")\n","증가한\n","29\n","##만\n","88\n","##20\n","##명으로\n","관련\n","통계\n","##를\n","작성한\n","198\n","##3년\n","이후\n","가장\n","많았\n","##으며\n",",\n","5년\n","연속\n","증가\n","##세를\n","보였다\n",".\n","통계\n","##청은\n","인구\n","구조의\n","고령\n","##화와\n","지난해\n","1\n","~\n","2월\n","유\n","##례가\n","드문\n","한\n","##파\n","등을\n","그\n","원인으로\n","꼽았다\n",".\n","지난해\n","조사\n","##망\n","##률\n","(\n","인구\n","10만\n","명\n","##당\n","사망\n","##자\n","수\n",")\n","역시\n","58\n","##2\n",".\n","5명\n","##으로\n","전년\n","##보다\n","4\n",".\n","5\n","%\n","(\n","25\n",".\n","1명\n",")\n","증가\n","##해\n","5년\n","연속\n","늘었다\n",".\n","특히\n","80\n","##세\n","이상의\n","사망\n","##자가\n","전체\n","사망\n","##자의\n","절반\n","##에\n","가까운\n","46\n",".\n","3\n","%\n","로\n",",\n","10년\n","전보다\n","14\n",".\n","3\n","%\n","포인트\n","(\n","p\n",")\n","나\n","증가했다\n",".\n","[UNK]\n","[UNK]\n","알\n","##츠\n","##하이\n","##머\n","##병\n","사망\n","##률\n","순위\n","[UNK]\n","껑\n","##충\n","[UNK]\n","사망\n","##원인\n","##별로\n","보면\n","지난해\n","암\n","(\n","악성\n","##신\n","##생물\n",")\n","에\n","의한\n","사망\n","##률\n","(\n","이하\n","인구\n","10만\n","명\n","##당\n","사망\n","##자\n","수\n",")\n","은\n","15\n","##4\n",".\n","3명\n","##으로\n","전년\n","##보다\n","0\n",".\n","2\n","%\n","증가했다\n",".\n","198\n","##3년\n","관련\n","통계\n","##를\n","집계\n","##한\n","이래\n","줄곧\n","암\n","##이\n","사망\n","##원인\n","1위\n","##로\n","집계됐다\n",".\n","특히\n","폐\n","##렴\n","(\n","[UNK]\n",")\n","과\n","치매\n","##의\n","일종\n","##인\n","알\n","##츠\n","##하이\n","##머\n","##병\n","(\n","[UNK]\n",")\n","에\n","의한\n","사망\n","##률\n","순위\n","상승\n","##이\n","두드러\n","##졌다\n",".\n","폐\n","##렴\n","사망\n","##률은\n","2004년\n","10\n","##위에서\n","꾸준히\n","순위\n","##가\n","상승\n","##하고\n","있고\n",",\n","알\n","##츠\n","##하이\n","##머\n","##병\n","사망\n","##률\n","역시\n","통계\n","작성\n","이래\n","10대\n","사인\n","##에\n","처음\n","포함\n","##됐다\n",".\n","지난해\n","알\n","##츠\n","##하이\n","##머\n","##병에\n","의한\n","사망\n","##률은\n","12\n",".\n","0\n","##명으로\n","전년\n","(\n","9\n",".\n","8\n","##명\n",")\n","대비\n","22\n",".\n","5\n","%\n","증가했다\n",".\n","알\n","##츠\n","##하이\n","##머\n","##병\n","사망\n","##률은\n","10년\n","전\n","(\n","3\n",".\n","8\n","##명\n",")\n","과\n","비교하면\n","무려\n","21\n","##4\n",".\n","2\n","%\n","증가했다\n",".\n","폐\n","##렴\n","사망\n","##률은\n","45\n",".\n","4명\n","##으로\n","전년\n","(\n","37\n",".\n","8\n","##명\n",")\n","대비\n","20\n",".\n","0\n","%\n","증가했다\n",".\n","알코올\n","관련\n","사망\n","##률은\n","9\n",".\n","6\n","##명으로\n","전년\n","##보다\n","2\n",".\n","0\n","%\n","늘었다\n",".\n","[UNK]\n","5년\n","만에\n","증가\n","[UNK]\n","[UNK]\n","베르\n","##테르\n","효과\n","영향\n","[UNK]\n","지난해\n","자살\n","##에\n","의한\n","사망\n","##자는\n","1만\n","36\n","##70\n","##명으로\n","전년\n","##보다\n","9\n",".\n","7\n","%\n","(\n","120\n","##7\n","##명\n",")\n","증가했다\n",".\n","자살\n","##률은\n","26\n",".\n","6\n","##명으로\n","전년\n","##보다\n","2\n",".\n","3명\n","(\n","[SEP]\n"]}]},{"cell_type":"code","metadata":{"id":"YLCXZqyt113F","executionInfo":{"status":"ok","timestamp":1633251994775,"user_tz":-540,"elapsed":13,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}}},"source":["outputs2=tokenizer('<bos>'+ex+'<eos>', \n","      truncation=True, max_length=512, padding=\"max_length\")"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FABTcdhc2AsU","executionInfo":{"status":"ok","timestamp":1633252071664,"user_tz":-540,"elapsed":305,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"4f84fddc-bf0d-4d14-8af0-389f1afb60ff"},"source":["print(outputs2.keys()) # token_type_ids????"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"]}]},{"cell_type":"markdown","metadata":{"id":"S_ZGKV03rqzz"},"source":["# Torch Dataset Customizing"]},{"cell_type":"code","metadata":{"id":"boRV7kCkvgek","executionInfo":{"status":"ok","timestamp":1633252362108,"user_tz":-540,"elapsed":309,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}}},"source":["import torch\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","torch.manual_seed(42)\n","\n","from transformers import GPT2LMHeadModel, GPT2Config\n","from transformers import AdamW, get_linear_schedule_with_warmup"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vl5of2gxyIoi","executionInfo":{"status":"ok","timestamp":1633252362424,"user_tz":-540,"elapsed":2,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}}},"source":["class BertSharedDataset(Dataset):\n","\n","  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=512): # gpt2_type 바꿔야 될듯\n","\n","    self.tokenizer = tokenizer\n","    self.input_ids = []\n","    self.attn_masks = []\n","\n","    for txt in txt_list: # 노래 별로 encode후, tensor로 만들어서 배열에 넣음\n","\n","      encodings_dict = tokenizer('<bos>'+ txt + '<eos>', \n","                                        truncation=True, max_length=max_length, \n","                                        padding=\"max_length\") \n","\n","      self.input_ids.append(torch.tensor(encodings_dict['input_ids'][1:])) # cls 제거\n","      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask'][1:])) # cls 제거\n","    \n","  def __len__(self):\n","    return len(self.input_ids)\n","\n","  def __getitem__(self, idx):\n","    return self.input_ids[idx], self.attn_masks[idx]"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9o05CQhfbP4F","executionInfo":{"status":"ok","timestamp":1633252995654,"user_tz":-540,"elapsed":583055,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"fc732861-136a-45a3-9dc9-c13628d5def4"},"source":["# 전처리한 article 리스트(df)에 대해 파이토치 데이터셋 생성\n","\n","dataset=BertSharedDataset(df,tokenizer,max_length=512)\n","\n","# Split into training and validation sets\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["234,627 training samples\n","26,070 validation samples\n"]}]},{"cell_type":"code","metadata":{"id":"7IZcGZjRbvGR"},"source":["train_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2MMkrtPbbwT0","executionInfo":{"status":"ok","timestamp":1633253382574,"user_tz":-540,"elapsed":378,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}}},"source":["# 토치 데이터 로더\n","batch_size = 2 # 서버에서 돌리면 다르게 설정해도 될듯\n","# GPT2 is a large model. Increasing the batch size above 2 has lead to out of memory problems. \n","# This can be mitigated by accumulating the gradients but that is out of scope here.\n","\n","# Create the DataLoaders for our training and validation datasets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJ-hJ2PJbzGI"},"source":["# Fine-Tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":513},"id":"qBnZUWUAb8e9","executionInfo":{"status":"error","timestamp":1633253392406,"user_tz":-540,"elapsed":3933,"user":{"displayName":"오홍민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil4g91xxJaBKfMiSfvW8r5-aVpUBhcP7pm9VCzZQ=s64","userId":"06247282495904803544"}},"outputId":"a24553bb-6c4c-419f-f5ef-aba2fd89995e"},"source":["# # # I'm not really doing anything with the config buheret\n","# configuration = GPT2Config.from_pretrained('kykim/bertshared-kor-base', output_hidden_states=False)\n","\n","# # instantiate the model\n","model = GPT2LMHeadModel.from_pretrained(\"kykim/bertshared-kor-base\") # 여기도 수정해야될 부분인가?"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type encoder-decoder to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n","Some weights of the model checkpoint at kykim/bertshared-kor-base were not used when initializing GPT2LMHeadModel: ['decoder.bert.encoder.layer.2.crossattention.self.query.weight', 'encoder.encoder.layer.11.attention.self.query.bias', 'decoder.bert.encoder.layer.5.crossattention.self.value.weight', 'encoder.encoder.layer.2.intermediate.dense.weight', 'encoder.encoder.layer.1.intermediate.dense.bias', 'decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.5.output.LayerNorm.weight', 'decoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.encoder.layer.10.attention.self.query.bias', 'decoder.bert.encoder.layer.7.attention.self.value.bias', 'decoder.bert.encoder.layer.4.attention.output.dense.weight', 'decoder.bert.embeddings.token_type_embeddings.weight', 'decoder.bert.encoder.layer.11.attention.self.key.bias', 'decoder.bert.encoder.layer.8.crossattention.self.query.bias', 'decoder.bert.encoder.layer.5.output.LayerNorm.bias', 'decoder.bert.encoder.layer.8.output.dense.bias', 'decoder.bert.encoder.layer.5.output.dense.weight', 'encoder.encoder.layer.5.attention.self.query.weight', 'encoder.encoder.layer.10.attention.self.value.weight', 'encoder.encoder.layer.2.attention.self.query.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.8.attention.self.key.weight', 'decoder.bert.encoder.layer.9.crossattention.self.value.bias', 'decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.self.value.weight', 'encoder.encoder.layer.3.output.LayerNorm.weight', 'decoder.bert.embeddings.position_ids', 'decoder.bert.encoder.layer.7.crossattention.self.value.bias', 'decoder.bert.encoder.layer.10.output.dense.weight', 'encoder.encoder.layer.8.intermediate.dense.bias', 'decoder.bert.encoder.layer.6.crossattention.self.key.weight', 'encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.encoder.layer.11.attention.self.value.bias', 'decoder.bert.encoder.layer.0.attention.self.value.weight', 'decoder.bert.encoder.layer.7.crossattention.self.key.weight', 'decoder.bert.encoder.layer.11.output.dense.bias', 'decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.7.output.LayerNorm.bias', 'decoder.bert.encoder.layer.11.attention.self.query.bias', 'decoder.bert.encoder.layer.11.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.encoder.layer.7.output.LayerNorm.bias', 'decoder.bert.encoder.layer.1.crossattention.self.query.weight', 'encoder.encoder.layer.0.attention.self.query.bias', 'decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.3.crossattention.self.query.bias', 'encoder.encoder.layer.3.attention.self.value.weight', 'decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.encoder.layer.6.attention.self.key.weight', 'decoder.bert.encoder.layer.10.attention.self.key.weight', 'decoder.bert.encoder.layer.7.crossattention.output.dense.bias', 'decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.embeddings.position_embeddings.weight', 'decoder.bert.encoder.layer.7.output.dense.bias', 'encoder.encoder.layer.0.output.LayerNorm.bias', 'encoder.encoder.layer.10.attention.self.key.weight', 'decoder.bert.encoder.layer.11.crossattention.output.dense.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.6.attention.self.query.bias', 'decoder.bert.encoder.layer.0.crossattention.self.value.weight', 'encoder.encoder.layer.3.attention.self.query.weight', 'decoder.bert.encoder.layer.8.attention.self.key.bias', 'decoder.bert.encoder.layer.4.output.dense.weight', 'decoder.bert.encoder.layer.10.crossattention.self.value.bias', 'decoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.encoder.layer.0.attention.self.key.weight', 'decoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.encoder.layer.9.output.LayerNorm.weight', 'encoder.encoder.layer.11.output.LayerNorm.weight', 'decoder.bert.encoder.layer.2.attention.output.dense.bias', 'decoder.cls.predictions.transform.LayerNorm.weight', 'decoder.bert.encoder.layer.8.crossattention.self.value.bias', 'decoder.bert.encoder.layer.8.crossattention.self.value.weight', 'decoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.encoder.layer.7.attention.self.key.bias', 'decoder.bert.encoder.layer.3.output.LayerNorm.bias', 'decoder.cls.predictions.bias', 'encoder.encoder.layer.7.attention.self.query.weight', 'decoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.encoder.layer.1.output.dense.bias', 'decoder.bert.encoder.layer.5.attention.self.query.weight', 'decoder.bert.encoder.layer.9.crossattention.output.dense.weight', 'decoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'decoder.bert.encoder.layer.1.crossattention.output.dense.weight', 'encoder.encoder.layer.8.attention.self.value.bias', 'decoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.encoder.layer.1.intermediate.dense.weight', 'encoder.encoder.layer.8.attention.output.dense.weight', 'decoder.bert.encoder.layer.4.crossattention.self.value.bias', 'decoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.encoder.layer.3.output.LayerNorm.bias', 'decoder.bert.encoder.layer.1.output.LayerNorm.bias', 'decoder.bert.encoder.layer.9.crossattention.output.dense.bias', 'decoder.bert.encoder.layer.2.attention.self.value.bias', 'decoder.bert.encoder.layer.0.attention.self.query.weight', 'decoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.encoder.layer.9.attention.output.dense.weight', 'decoder.bert.encoder.layer.2.intermediate.dense.weight', 'decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.crossattention.self.key.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.output.dense.bias', 'decoder.bert.encoder.layer.4.output.LayerNorm.bias', 'decoder.bert.encoder.layer.3.attention.output.dense.weight', 'decoder.bert.encoder.layer.1.crossattention.output.dense.bias', 'decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.encoder.layer.7.attention.self.key.weight', 'encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.1.crossattention.self.key.bias', 'encoder.encoder.layer.4.attention.output.dense.bias', 'decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.7.intermediate.dense.bias', 'decoder.bert.encoder.layer.8.intermediate.dense.weight', 'decoder.bert.encoder.layer.6.crossattention.self.value.bias', 'encoder.encoder.layer.2.output.dense.weight', 'encoder.encoder.layer.10.output.LayerNorm.weight', 'encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.encoder.layer.9.attention.self.value.weight', 'encoder.encoder.layer.9.output.dense.weight', 'decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.7.output.LayerNorm.weight', 'decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.3.crossattention.self.value.bias', 'encoder.encoder.layer.10.output.dense.bias', 'decoder.bert.encoder.layer.9.attention.self.query.weight', 'decoder.bert.encoder.layer.10.crossattention.self.value.weight', 'encoder.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.10.attention.self.value.bias', 'decoder.bert.encoder.layer.3.output.dense.bias', 'decoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.encoder.layer.4.attention.self.key.bias', 'decoder.bert.encoder.layer.11.crossattention.self.value.weight', 'encoder.encoder.layer.5.attention.self.key.bias', 'encoder.encoder.layer.0.output.dense.weight', 'decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.encoder.layer.6.attention.self.value.bias', 'encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.1.intermediate.dense.bias', 'decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.encoder.layer.11.attention.self.value.weight', 'decoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'decoder.bert.encoder.layer.6.intermediate.dense.bias', 'decoder.bert.encoder.layer.0.crossattention.self.query.weight', 'encoder.encoder.layer.3.attention.self.query.bias', 'decoder.bert.encoder.layer.10.crossattention.self.query.weight', 'decoder.bert.encoder.layer.9.attention.output.dense.bias', 'decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.encoder.layer.0.intermediate.dense.weight', 'decoder.bert.encoder.layer.2.crossattention.self.value.weight', 'decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'decoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.encoder.layer.1.attention.self.query.bias', 'decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.9.output.dense.weight', 'decoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.encoder.layer.4.attention.self.key.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.11.crossattention.self.value.bias', 'decoder.bert.encoder.layer.9.crossattention.self.query.weight', 'decoder.bert.encoder.layer.9.crossattention.self.value.weight', 'decoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.encoder.layer.9.attention.self.query.bias', 'decoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.encoder.layer.3.attention.output.dense.weight', 'decoder.bert.encoder.layer.11.crossattention.self.key.weight', 'decoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.encoder.layer.3.intermediate.dense.bias', 'encoder.encoder.layer.2.output.LayerNorm.bias', 'encoder.encoder.layer.5.intermediate.dense.weight', 'decoder.bert.encoder.layer.1.crossattention.self.value.weight', 'encoder.encoder.layer.1.attention.self.key.bias', 'encoder.encoder.layer.10.attention.output.dense.weight', 'decoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.encoder.layer.6.output.dense.weight', 'decoder.bert.encoder.layer.6.crossattention.self.query.bias', 'encoder.encoder.layer.2.attention.output.dense.bias', 'encoder.encoder.layer.5.output.dense.weight', 'decoder.bert.encoder.layer.3.attention.self.value.weight', 'decoder.bert.encoder.layer.8.attention.output.dense.weight', 'decoder.bert.encoder.layer.1.crossattention.self.query.bias', 'encoder.encoder.layer.0.attention.self.key.bias', 'decoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'encoder.encoder.layer.8.output.dense.bias', 'decoder.bert.encoder.layer.9.intermediate.dense.bias', 'decoder.bert.encoder.layer.5.attention.self.key.bias', 'decoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.encoder.layer.11.attention.self.key.weight', 'decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.encoder.layer.11.attention.output.dense.weight', 'decoder.bert.encoder.layer.10.attention.self.query.bias', 'decoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.encoder.layer.9.attention.output.dense.bias', 'decoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.encoder.layer.11.attention.self.key.bias', 'encoder.encoder.layer.11.attention.output.dense.bias', 'encoder.encoder.layer.0.attention.self.query.weight', 'encoder.encoder.layer.10.attention.self.key.bias', 'decoder.bert.encoder.layer.7.attention.self.key.bias', 'decoder.bert.embeddings.position_embeddings.weight', 'encoder.encoder.layer.6.output.dense.bias', 'encoder.encoder.layer.1.attention.output.dense.weight', 'decoder.bert.encoder.layer.9.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.encoder.layer.8.attention.self.key.weight', 'encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.9.output.LayerNorm.weight', 'decoder.bert.encoder.layer.1.attention.output.dense.bias', 'decoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.encoder.layer.4.output.dense.weight', 'encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.output.dense.weight', 'decoder.bert.encoder.layer.4.crossattention.self.key.weight', 'encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.crossattention.self.query.bias', 'decoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.encoder.layer.2.intermediate.dense.bias', 'decoder.bert.encoder.layer.1.attention.self.key.bias', 'decoder.bert.encoder.layer.2.crossattention.self.query.bias', 'decoder.bert.encoder.layer.5.crossattention.self.key.weight', 'decoder.bert.encoder.layer.10.intermediate.dense.weight', 'decoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.encoder.layer.11.output.dense.bias', 'decoder.bert.encoder.layer.3.crossattention.self.query.weight', 'encoder.encoder.layer.5.output.LayerNorm.bias', 'decoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.intermediate.dense.bias', 'decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.encoder.layer.6.attention.self.value.weight', 'decoder.bert.encoder.layer.1.attention.self.query.bias', 'decoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.encoder.layer.9.output.dense.bias', 'decoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.0.intermediate.dense.bias', 'decoder.bert.encoder.layer.4.crossattention.self.query.weight', 'decoder.bert.encoder.layer.6.output.LayerNorm.bias', 'decoder.bert.encoder.layer.7.crossattention.self.query.bias', 'encoder.encoder.layer.4.output.LayerNorm.bias', 'decoder.bert.encoder.layer.5.attention.self.value.bias', 'decoder.bert.encoder.layer.5.crossattention.self.key.bias', 'encoder.encoder.layer.3.attention.self.key.bias', 'encoder.encoder.layer.4.attention.self.query.bias', 'decoder.bert.encoder.layer.6.crossattention.self.query.weight', 'encoder.encoder.layer.5.intermediate.dense.bias', 'decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.3.attention.self.key.bias', 'decoder.bert.encoder.layer.6.output.dense.bias', 'encoder.encoder.layer.4.attention.self.query.weight', 'encoder.encoder.layer.2.attention.self.value.bias', 'encoder.encoder.layer.5.attention.self.value.weight', 'decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.encoder.layer.9.attention.self.key.weight', 'encoder.encoder.layer.10.intermediate.dense.weight', 'encoder.pooler.dense.bias', 'decoder.bert.encoder.layer.10.output.LayerNorm.weight', 'decoder.bert.encoder.layer.7.attention.output.dense.weight', 'decoder.bert.embeddings.LayerNorm.bias', 'encoder.encoder.layer.5.attention.self.key.weight', 'decoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.encoder.layer.0.attention.output.dense.weight', 'encoder.embeddings.LayerNorm.weight', 'decoder.bert.encoder.layer.0.output.dense.bias', 'decoder.bert.encoder.layer.3.attention.self.query.weight', 'decoder.bert.encoder.layer.11.attention.output.dense.weight', 'decoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.encoder.layer.6.attention.self.query.weight', 'encoder.encoder.layer.8.intermediate.dense.weight', 'decoder.bert.encoder.layer.9.crossattention.self.key.weight', 'encoder.encoder.layer.2.attention.self.key.weight', 'decoder.bert.encoder.layer.11.intermediate.dense.weight', 'decoder.bert.embeddings.LayerNorm.weight', 'decoder.bert.encoder.layer.11.attention.self.query.weight', 'decoder.bert.encoder.layer.1.attention.self.key.weight', 'decoder.bert.encoder.layer.4.intermediate.dense.weight', 'decoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.encoder.layer.11.attention.self.query.weight', 'decoder.bert.encoder.layer.11.crossattention.output.dense.bias', 'encoder.embeddings.LayerNorm.bias', 'decoder.bert.encoder.layer.8.crossattention.self.query.weight', 'decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.5.attention.output.dense.weight', 'decoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.encoder.layer.7.attention.self.query.bias', 'encoder.encoder.layer.8.attention.output.dense.bias', 'decoder.bert.encoder.layer.6.attention.self.key.bias', 'decoder.bert.encoder.layer.2.output.dense.weight', 'encoder.encoder.layer.4.output.LayerNorm.weight', 'encoder.encoder.layer.0.output.dense.bias', 'encoder.encoder.layer.7.intermediate.dense.weight', 'decoder.bert.encoder.layer.7.output.dense.weight', 'encoder.encoder.layer.4.intermediate.dense.weight', 'decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.11.attention.self.value.bias', 'decoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.encoder.layer.6.intermediate.dense.weight', 'decoder.bert.encoder.layer.3.crossattention.self.key.bias', 'encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.3.crossattention.self.value.weight', 'encoder.encoder.layer.10.output.dense.weight', 'decoder.bert.encoder.layer.1.attention.self.query.weight', 'decoder.bert.encoder.layer.8.output.dense.weight', 'encoder.encoder.layer.2.output.LayerNorm.weight', 'encoder.encoder.layer.8.attention.self.value.weight', 'decoder.bert.encoder.layer.2.output.dense.bias', 'encoder.encoder.layer.10.attention.self.query.weight', 'decoder.bert.encoder.layer.3.intermediate.dense.bias', 'decoder.bert.encoder.layer.7.crossattention.output.dense.weight', 'encoder.encoder.layer.1.attention.self.key.weight', 'encoder.encoder.layer.2.output.dense.bias', 'decoder.bert.encoder.layer.6.attention.self.value.weight', 'decoder.bert.encoder.layer.11.crossattention.self.query.bias', 'decoder.bert.encoder.layer.5.crossattention.output.dense.bias', 'decoder.bert.encoder.layer.6.crossattention.self.value.weight', 'encoder.encoder.layer.7.attention.self.value.bias', 'decoder.bert.encoder.layer.0.crossattention.self.key.weight', 'decoder.bert.encoder.layer.2.output.LayerNorm.weight', 'decoder.bert.encoder.layer.7.intermediate.dense.weight', 'decoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.encoder.layer.1.output.LayerNorm.bias', 'encoder.encoder.layer.2.attention.self.query.bias', 'encoder.encoder.layer.5.attention.output.dense.weight', 'encoder.encoder.layer.1.output.LayerNorm.weight', 'decoder.bert.encoder.layer.5.crossattention.output.dense.weight', 'decoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'encoder.encoder.layer.3.intermediate.dense.weight', 'encoder.encoder.layer.9.intermediate.dense.weight', 'decoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.encoder.layer.3.attention.self.value.bias', 'decoder.bert.encoder.layer.5.intermediate.dense.weight', 'decoder.bert.encoder.layer.6.crossattention.self.key.bias', 'encoder.embeddings.position_ids', 'decoder.bert.encoder.layer.8.attention.self.query.bias', 'decoder.bert.encoder.layer.4.output.dense.bias', 'encoder.encoder.layer.2.attention.output.dense.weight', 'decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.1.output.dense.weight', 'encoder.encoder.layer.3.output.dense.bias', 'encoder.encoder.layer.1.output.dense.weight', 'decoder.bert.encoder.layer.9.output.dense.bias', 'encoder.encoder.layer.4.intermediate.dense.bias', 'encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.5.crossattention.self.value.bias', 'decoder.bert.encoder.layer.1.crossattention.self.value.bias', 'decoder.bert.encoder.layer.6.attention.output.dense.bias', 'decoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.encoder.layer.6.output.LayerNorm.weight', 'encoder.encoder.layer.11.intermediate.dense.weight', 'decoder.bert.encoder.layer.4.crossattention.self.value.weight', 'encoder.encoder.layer.5.attention.self.query.bias', 'decoder.bert.encoder.layer.8.crossattention.self.key.weight', 'encoder.encoder.layer.6.attention.self.key.bias', 'encoder.encoder.layer.7.attention.output.dense.bias', 'decoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.encoder.layer.3.attention.output.dense.bias', 'encoder.encoder.layer.4.attention.self.value.weight', 'decoder.bert.encoder.layer.2.attention.self.query.weight', 'decoder.bert.encoder.layer.4.attention.output.dense.bias', 'decoder.bert.encoder.layer.11.crossattention.self.query.weight', 'encoder.encoder.layer.8.output.LayerNorm.bias', 'decoder.bert.encoder.layer.10.output.dense.bias', 'decoder.bert.encoder.layer.9.crossattention.self.key.bias', 'decoder.bert.encoder.layer.10.crossattention.self.key.weight', 'decoder.bert.encoder.layer.3.crossattention.output.dense.weight', 'encoder.encoder.layer.11.output.dense.weight', 'decoder.bert.encoder.layer.2.crossattention.self.key.weight', 'encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.encoder.layer.0.attention.self.value.bias', 'encoder.encoder.layer.7.output.LayerNorm.weight', 'decoder.bert.encoder.layer.5.crossattention.self.query.bias', 'encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.encoder.layer.0.attention.output.dense.bias', 'decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.6.attention.output.dense.weight', 'decoder.bert.encoder.layer.1.output.dense.bias', 'encoder.pooler.dense.weight', 'encoder.encoder.layer.5.output.dense.bias', 'decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.encoder.layer.10.attention.output.dense.bias', 'encoder.encoder.layer.10.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.0.crossattention.self.value.bias', 'decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.2.intermediate.dense.bias', 'decoder.bert.encoder.layer.3.crossattention.self.key.weight', 'decoder.bert.encoder.layer.11.output.dense.weight', 'decoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'encoder.encoder.layer.8.output.LayerNorm.weight', 'decoder.bert.encoder.layer.9.attention.self.query.bias', 'decoder.cls.predictions.decoder.weight', 'decoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.encoder.layer.6.attention.self.query.bias', 'encoder.encoder.layer.1.attention.self.query.weight', 'decoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.encoder.layer.1.attention.self.value.weight', 'decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'decoder.cls.predictions.transform.LayerNorm.bias', 'decoder.bert.encoder.layer.5.crossattention.self.query.weight', 'encoder.encoder.layer.4.output.dense.bias', 'decoder.bert.encoder.layer.8.output.LayerNorm.bias', 'decoder.cls.predictions.decoder.bias', 'decoder.bert.encoder.layer.3.crossattention.output.dense.bias', 'encoder.encoder.layer.0.intermediate.dense.bias', 'decoder.bert.encoder.layer.10.attention.output.dense.bias', 'decoder.bert.encoder.layer.7.crossattention.self.key.bias', 'decoder.bert.encoder.layer.4.attention.self.value.weight', 'decoder.bert.encoder.layer.9.crossattention.self.query.bias', 'encoder.embeddings.word_embeddings.weight', 'decoder.bert.encoder.layer.8.attention.output.dense.bias', 'decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.5.attention.output.dense.bias', 'decoder.bert.encoder.layer.7.attention.self.query.bias', 'decoder.bert.encoder.layer.3.output.LayerNorm.weight', 'decoder.bert.encoder.layer.4.attention.self.query.weight', 'decoder.bert.encoder.layer.6.output.LayerNorm.weight', 'decoder.bert.encoder.layer.0.attention.self.value.bias', 'decoder.bert.encoder.layer.4.attention.self.query.bias', 'decoder.bert.encoder.layer.6.attention.self.value.bias', 'decoder.bert.encoder.layer.5.output.dense.bias', 'decoder.bert.encoder.layer.0.crossattention.self.key.bias', 'decoder.cls.predictions.transform.dense.bias', 'encoder.encoder.layer.11.output.LayerNorm.bias', 'decoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'decoder.bert.encoder.layer.8.attention.self.query.weight', 'decoder.bert.encoder.layer.11.intermediate.dense.bias', 'decoder.bert.encoder.layer.3.attention.output.dense.bias', 'decoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.encoder.layer.1.attention.self.value.bias', 'decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.6.output.dense.weight', 'decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.7.crossattention.self.query.weight', 'decoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.encoder.layer.6.attention.output.dense.bias', 'decoder.bert.encoder.layer.10.attention.self.value.weight', 'decoder.bert.encoder.layer.9.attention.output.dense.weight', 'decoder.bert.encoder.layer.11.output.LayerNorm.weight', 'decoder.bert.encoder.layer.2.crossattention.self.key.bias', 'decoder.bert.encoder.layer.11.crossattention.self.key.bias', 'decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.attention.output.dense.weight', 'decoder.bert.encoder.layer.8.crossattention.self.key.bias', 'decoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.8.attention.self.key.bias', 'encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.encoder.layer.3.output.dense.weight', 'encoder.encoder.layer.6.attention.output.dense.weight', 'encoder.encoder.layer.1.attention.output.dense.bias', 'decoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'decoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.encoder.layer.5.attention.self.value.bias', 'decoder.bert.encoder.layer.2.crossattention.self.value.bias', 'decoder.bert.encoder.layer.10.crossattention.self.key.bias', 'encoder.encoder.layer.6.intermediate.dense.bias', 'encoder.encoder.layer.9.output.LayerNorm.bias', 'encoder.encoder.layer.6.output.LayerNorm.bias', 'decoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.encoder.layer.7.output.dense.weight', 'encoder.encoder.layer.7.intermediate.dense.bias', 'decoder.bert.encoder.layer.10.crossattention.self.query.bias', 'decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.encoder.layer.8.attention.self.query.bias', 'decoder.bert.encoder.layer.10.attention.self.value.bias', 'decoder.bert.encoder.layer.10.attention.self.query.weight', 'decoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.encoder.layer.7.output.dense.bias', 'decoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.encoder.layer.11.intermediate.dense.bias', 'decoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.embeddings.token_type_embeddings.weight', 'decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.encoder.layer.9.attention.self.key.bias', 'encoder.encoder.layer.2.attention.self.key.bias', 'decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.encoder.layer.9.intermediate.dense.bias', 'decoder.cls.predictions.transform.dense.weight', 'decoder.bert.encoder.layer.4.crossattention.self.query.bias', 'decoder.bert.encoder.layer.11.attention.self.key.weight', 'decoder.bert.embeddings.word_embeddings.weight', 'encoder.encoder.layer.9.attention.self.value.bias', 'decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.encoder.layer.7.attention.output.dense.weight', 'encoder.encoder.layer.8.attention.self.query.weight', 'decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.encoder.layer.4.attention.self.value.bias', 'encoder.encoder.layer.5.output.LayerNorm.weight', 'decoder.bert.encoder.layer.7.crossattention.self.value.weight', 'encoder.encoder.layer.0.output.LayerNorm.weight', 'encoder.encoder.layer.3.attention.self.key.weight', 'decoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.encoder.layer.9.attention.self.query.weight', 'encoder.encoder.layer.8.output.dense.weight', 'encoder.encoder.layer.10.intermediate.dense.bias', 'decoder.bert.encoder.layer.1.crossattention.self.key.weight', 'decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.3.output.dense.weight', 'encoder.encoder.layer.7.attention.self.value.weight']\n","- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at kykim/bertshared-kor-base and are newly initialized: ['h.8.ln_1.bias', 'h.8.attn.c_attn.weight', 'h.11.mlp.c_proj.bias', 'h.6.attn.c_proj.bias', 'h.7.mlp.c_proj.bias', 'h.9.mlp.c_proj.weight', 'h.7.attn.c_attn.weight', 'h.0.attn.c_attn.weight', 'h.9.attn.c_attn.weight', 'h.0.attn.c_proj.weight', 'h.7.mlp.c_fc.weight', 'h.7.ln_2.weight', 'h.7.ln_1.weight', 'h.9.mlp.c_proj.bias', 'h.9.mlp.c_fc.bias', 'h.2.ln_1.bias', 'h.9.ln_1.bias', 'h.7.ln_2.bias', 'h.1.ln_2.weight', 'h.5.ln_2.bias', 'h.7.mlp.c_fc.bias', 'h.1.attn.c_proj.weight', 'h.11.attn.c_attn.weight', 'h.3.mlp.c_fc.bias', 'h.10.ln_2.bias', 'h.2.ln_2.bias', 'h.1.mlp.c_proj.bias', 'h.4.ln_1.bias', 'h.8.attn.c_proj.weight', 'h.11.mlp.c_proj.weight', 'h.0.mlp.c_fc.bias', 'h.3.attn.c_attn.weight', 'h.9.ln_2.weight', 'h.0.ln_1.bias', 'h.5.attn.c_proj.bias', 'h.6.ln_1.bias', 'h.6.mlp.c_fc.bias', 'h.4.ln_2.weight', 'wpe.weight', 'h.4.mlp.c_fc.bias', 'h.4.attn.c_proj.bias', 'h.4.ln_2.bias', 'h.0.ln_2.weight', 'h.1.mlp.c_proj.weight', 'h.10.ln_1.weight', 'h.6.ln_1.weight', 'h.5.attn.c_attn.weight', 'h.11.ln_2.bias', 'h.1.attn.c_proj.bias', 'h.4.attn.c_attn.weight', 'h.4.attn.c_proj.weight', 'h.0.mlp.c_proj.bias', 'h.11.attn.c_proj.weight', 'h.2.mlp.c_proj.weight', 'h.3.ln_2.bias', 'h.2.attn.c_proj.weight', 'h.4.ln_1.weight', 'h.10.attn.c_proj.weight', 'h.0.ln_1.weight', 'h.7.attn.c_proj.bias', 'h.2.ln_1.weight', 'h.6.mlp.c_fc.weight', 'h.3.attn.c_proj.weight', 'h.7.mlp.c_proj.weight', 'h.11.attn.c_proj.bias', 'h.10.mlp.c_proj.weight', 'h.10.ln_2.weight', 'h.11.ln_1.weight', 'h.7.ln_1.bias', 'h.3.ln_1.bias', 'h.6.attn.c_proj.weight', 'h.8.ln_2.weight', 'h.3.mlp.c_proj.weight', 'h.3.attn.c_proj.bias', 'h.1.attn.c_attn.weight', 'h.10.attn.c_proj.bias', 'h.2.ln_2.weight', 'wte.weight', 'h.5.ln_1.bias', 'h.1.ln_1.weight', 'h.5.mlp.c_fc.weight', 'h.4.mlp.c_fc.weight', 'h.8.attn.c_proj.bias', 'h.2.mlp.c_proj.bias', 'h.1.ln_2.bias', 'h.5.attn.c_proj.weight', 'h.9.ln_2.bias', 'h.3.ln_1.weight', 'h.10.mlp.c_fc.bias', 'ln_f.bias', 'h.8.mlp.c_proj.weight', 'h.8.ln_2.bias', 'h.10.mlp.c_proj.bias', 'h.0.mlp.c_proj.weight', 'h.10.mlp.c_fc.weight', 'h.5.mlp.c_fc.bias', 'h.4.mlp.c_proj.bias', 'h.6.mlp.c_proj.weight', 'h.5.mlp.c_proj.bias', 'h.1.mlp.c_fc.bias', 'h.9.ln_1.weight', 'h.0.attn.c_proj.bias', 'h.6.ln_2.bias', 'h.9.attn.c_proj.bias', 'h.7.attn.c_proj.weight', 'h.0.ln_2.bias', 'h.6.ln_2.weight', 'h.2.attn.c_attn.weight', 'h.3.mlp.c_proj.bias', 'h.3.ln_2.weight', 'h.8.mlp.c_proj.bias', 'h.3.mlp.c_fc.weight', 'h.8.ln_1.weight', 'h.6.attn.c_attn.weight', 'h.5.mlp.c_proj.weight', 'h.1.ln_1.bias', 'h.11.mlp.c_fc.weight', 'h.2.attn.c_proj.bias', 'h.2.mlp.c_fc.bias', 'h.4.mlp.c_proj.weight', 'h.10.attn.c_attn.weight', 'h.9.attn.c_proj.weight', 'h.5.ln_1.weight', 'h.10.ln_1.bias', 'h.2.mlp.c_fc.weight', 'h.8.mlp.c_fc.bias', 'h.9.mlp.c_fc.weight', 'h.11.mlp.c_fc.bias', 'h.11.ln_2.weight', 'ln_f.weight', 'h.8.mlp.c_fc.weight', 'h.6.mlp.c_proj.bias', 'h.1.mlp.c_fc.weight', 'h.0.mlp.c_fc.weight', 'h.5.ln_2.weight', 'h.11.ln_1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-5a9c2608cc47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# # instantiate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kykim/bertshared-kor-base\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 여기도 수정해야될 부분인가?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;31m# make sure token embedding weights are still tied if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtie_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0;31m# Set model in evaluation mode to deactivate DropOut modules by default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mtie_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tie_encoder_decoder_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1131\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'GPT2Model' object has no attribute 'encoder'"]}]},{"cell_type":"code","metadata":{"id":"q2oCiNeDcQAL"},"source":["# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n","# otherwise the tokenizer and model tensors won't match up\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Tell pytorch to run this model on the GPU.\n","device = torch.device(\"cuda\")\n","model.cuda()\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOJg2MDncZni"},"source":["\n","# some parameters I cooked up that work reasonably well\n","\n","epochs = 5\n","learning_rate = 5e-4\n","warmup_steps = 1e2\n","epsilon = 1e-8\n","\n","# this produces sample output every 100 steps \n","#예로 사용한 데이터셋이 작아서 sample_every : 문장생성을  100 -> 20step 마다로 바꿔봤음!\n","sample_every = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ByYnmQxkcbHC"},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","optimizer = AdamW(model.parameters(),\n","                  lr = learning_rate,\n","                  eps = epsilon\n","                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w8jQdc2OccST"},"source":["# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","# 학습 진행되면서 learning_rate 바뀌면서 학습 빠르게 진행되도록 해줌.\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = warmup_steps, \n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhPUfKq0cdUW"},"source":["def format_time(elapsed):\n","    return str(datetime.timedelta(seconds=int(round((elapsed)))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rTIfx6q-cePY"},"source":["for step, batch in enumerate(train_dataloader):\n","  print(step,len(batch),'batch[0] :',batch[0],'batch[1] :',batch[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cvxgiIywcg0u"},"source":["total_t0 = time.time()\n","\n","training_stats = []\n","\n","model = model.to(device) \n","\n","for epoch_i in range(0, epochs):\n","\n","    # ========================================\n","    #               Training\n","    # ========================================\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    t0 = time.time()\n","\n","    total_train_loss = 0\n","\n","    model.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","\n","        b_input_ids = batch[0].to(device) # GPU 입력으로 사용될 tensor는 모두 to(device) 필요\n","        b_labels = batch[0].to(device) # ? 라벨? 마스크?\n","        b_masks = batch[1].to(device) # ? \n","\n","        model.zero_grad()        # 변화도(Gradient) 매개변수를 0으로 만들고\n","\n","        outputs = model(  b_input_ids,  # loss를 출력하는지 확인.\n","                          labels=b_labels, \n","                          attention_mask = b_masks,\n","                          token_type_ids=None \n","                        )\n"," \n","        loss = outputs[0]  \n","        batch_loss = loss.item()\n","        total_train_loss += batch_loss\n","\n","        # Get sample every x batches.\n","        if step % sample_every == 0 and not step == 0:\n","\n","            elapsed = format_time(time.time() - t0) # elapsed : 지난 시간 출력.\n","            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n","\n","            model.eval() # 학습 반영안되도록. generate 할때는? 잘 모르겠음.\n","\n","            sample_outputs = model.generate( # 세대 토큰 넣어서 해봄, 눌러서 살펴보면 어떻게 쓰는지 나옴\n","                                    bos_token_id=random.randint(1,30000),  # The id of the beginning-of-sequence token. 랜덤한 단어로 시작하는 것.\n","                                    do_sample=True,# Whether or not to use sampling ; use greedy decoding   \n","                                    top_k=50, # The number of highest probability vocabulary  tokens to\n","                                    max_length = 200, # The maximum length of the sequence to be generated.\n","                                    top_p=0.95, # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n","                                    num_return_sequences=1, # The number of independently computed returned sequences for each element in the batch.\n","                                    repetition_penalty=2.0, \n","                                \n","                                )\n","            for i, sample_output in enumerate(sample_outputs):\n","                  print(\"{}: {}\".format(i, tokenizer_gpt3.decode(sample_output, skip_special_tokens=True))) # id를 decode해서 문장으로 출력\n","            \n","            model.train() # train은 일정 sample_every step마다 하는듯.\n","        \n","        # 여긴 뭐 학습하는과정? 역전파 등등.\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)       \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    model.eval()\n","\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","        \n","        with torch.no_grad():     # 학습 반영안되도록.\n","\n","            outputs  = model(b_input_ids, \n","#                            token_type_ids=None, \n","                             attention_mask = b_masks,\n","                            labels=b_labels)\n","          \n","            loss = outputs[0]  \n","            \n","        batch_loss = loss.item()\n","        total_eval_loss += batch_loss        \n","\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    \n","    validation_time = format_time(time.time() - t0)    \n","\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append( # 바로 밑에서 쓰임. 정확도가 올라가는지 시각화할때 사용할듯\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VrKsUAKej4Sb"},"source":["# Reference"]},{"cell_type":"markdown","metadata":{"id":"sO3WwXogj6je"},"source":["* [Hugging Face Tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) : \n","\n","* [Fine-tuning a pretrained model](https://huggingface.co/transformers/training.html)\n","\n","* [LMKor : kykim/bertshared-kor-base](https://github.com/kiyoungkim1/LMkor)\n","\n","* [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/pdf/1907.12461.pdf)\n","\n","* [가사생성](https://github.com/honghyeong/NLP_Creators/blob/main/code/fine-tuning/Indie_fine_tuning.ipynb)"]},{"cell_type":"code","metadata":{"id":"ZKShmyxPj5f4"},"source":[""],"execution_count":null,"outputs":[]}]}