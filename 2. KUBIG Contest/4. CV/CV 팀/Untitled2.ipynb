{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c1960ef-eead-4700-a593-0ad812f20e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6334\n",
      "(1334, 12)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pydicom\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import pydicom\n",
    "import glob\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "#import gdcm\n",
    "import cv2\n",
    "import warnings\n",
    "from fastai.vision.all import *\n",
    "from fastai.medical.imaging import *\n",
    "warnings.filterwarnings('ignore')\n",
    "dataset_path = Path('/home/sangjun.park/siim-covid19-detection')\n",
    "import vtk\n",
    "# numba\n",
    "import numba\n",
    "from numba import jit\n",
    "from vtk.util import numpy_support\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import pydicom\n",
    "import warnings\n",
    "from torchvision.models.detection import backbone_utils\n",
    "from torchvision.models.detection.backbone_utils import _validate_trainable_layers\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from pydicom import dcmread\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "paddingSize= 0\n",
    "    \n",
    "\n",
    "\n",
    "reader = vtk.vtkDICOMImageReader()\n",
    "\n",
    "\n",
    "# Thanks https://www.kaggle.com/tanlikesmath/siim-covid-19-detection-a-simple-eda\n",
    "def dicom2array(path, voi_lut=True, fix_monochrome=True):\n",
    "    # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n",
    "    dicom = pydicom.read_file(path)\n",
    "    \n",
    "    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n",
    "    # \"human-friendly\" view\n",
    "    if voi_lut:\n",
    "        data = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "    else:\n",
    "        data = dicom.pixel_array\n",
    "               \n",
    "    # depending on this value, X-ray may look inverted - fix that:\n",
    "    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        data = np.amax(data) - data\n",
    "        \n",
    "    data = data - np.min(data)\n",
    "    data = data / np.max(data)\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "    \n",
    "def plot_img(img, size=(7, 7), is_rgb=True, title=\"\", cmap='gray'):\n",
    "    plt.figure(figsize=size)\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_imgs(imgs, cols=4, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(500,500)):\n",
    "    rows = len(imgs)//cols + 1\n",
    "    fig = plt.figure(figsize=(cols*size, rows*size))\n",
    "    for i, img in enumerate(imgs):\n",
    "        if img_size is not None:\n",
    "            img = cv2.resize(img, img_size)\n",
    "        fig.add_subplot(rows, cols, i+1)\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def image_path(row):\n",
    "    study_path = dataset_path/'train'/row.StudyInstanceUID\n",
    "    for i in get_dicom_files(study_path):\n",
    "        if row.id.split('_')[0] == i.stem: return i \n",
    "        \n",
    "\n",
    "\n",
    "class Config:\n",
    "    n_folds: int = 5\n",
    "    seed: int = 2020\n",
    "    num_classes: int = 2\n",
    "    img_size: int = 256\n",
    "    fold_num: int = 0\n",
    "    device: str = 'cuda:0'\n",
    "\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(Config.seed)\n",
    "\n",
    "train_image_df = pd.read_csv('/home/sangjun.park/siim-covid19-detection/train_image_level.csv')\n",
    "\n",
    "\n",
    "# Thanks https://www.kaggle.com/tanlikesmath/siim-covid-19-detection-a-simple-eda\n",
    "train_image_df['class'] = train_image_df.label.apply(lambda x: x.split()[0])\n",
    "\n",
    "\n",
    "train_image_df['x_min'] = train_image_df.label.apply(lambda x: float(x.split()[2]))\n",
    "train_image_df['y_min'] = train_image_df.label.apply(lambda x: float(x.split()[3]))\n",
    "train_image_df['x_max'] = train_image_df.label.apply(lambda x: float(x.split()[4]))\n",
    "train_image_df['y_max'] = train_image_df.label.apply(lambda x: float(x.split()[5]))\n",
    "\n",
    "\n",
    "\n",
    "def get_bbox_area(row):\n",
    "    return (row['x_max']-row['x_min'])*(row['y_max']-row['y_min'])\n",
    "\n",
    "\n",
    "train_image_df['bbox_area'] = train_image_df.apply(get_bbox_area, axis=1)\n",
    "\n",
    "train_image_df['image_path'] = train_image_df.apply(image_path, axis=1)\n",
    "\n",
    "train_image_df['bbox_area'].hist()\n",
    "\n",
    "imgs = []\n",
    "image_paths = train_image_df['image_path'].values\n",
    "class_ids = train_image_df['class']\n",
    "\n",
    "# map label_id to specify color\n",
    "label2color = {class_id:[random.randint(0,255) for i in range(3)] for class_id in class_ids}\n",
    "thickness = 3\n",
    "scale = 5\n",
    "\n",
    "\n",
    "image_ids = train_image_df['id'].unique()\n",
    "valid_ids = image_ids[-5000:]# Tran and Validation Split \n",
    "train_ids = image_ids[:-5000]\n",
    "\n",
    "\n",
    "valid_df = train_image_df[train_image_df['id'].isin(valid_ids)]\n",
    "train_df = train_image_df[train_image_df['id'].isin(train_ids)]\n",
    "\n",
    "train_df[\"class_id\"] = [1]*len(train_df)\n",
    "valid_df[\"class_id\"] = [1]*len(valid_df)\n",
    "print(len(train_image_df))\n",
    "print(train_df.shape)\n",
    "train_df.head()\n",
    "\n",
    "class COVIDTrainDataLoader(Dataset): #Class to load Training Data\n",
    "    \n",
    "    def __init__(self, dataframe, transforms=None,stat = 'Train'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_ids = dataframe[\"id\"].unique()\n",
    "        \n",
    "        self.df = dataframe\n",
    "        self.transforms = transforms\n",
    "        self.stat = stat\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.stat == 'Train':\n",
    "            \n",
    "            image_id = self.image_ids[index]\n",
    "            \n",
    "            records = self.df[(self.df['id'] == image_id)]\n",
    "            records = records.reset_index(drop=True)\n",
    "            image = dicom2array(self.df[(self.df['id'] == image_id)]['image_path'].values[0])#dcmread\n",
    "\n",
    "            #image = ds.pixel_array\n",
    "           \n",
    "            '''if \"PhotometricInterpretation\" in dicom:\n",
    "                if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "                    image = np.amax(image) - image'''\n",
    "\n",
    "            intercept =  0.0\n",
    "            slope =1.0\n",
    "\n",
    "            if slope != 1:\n",
    "                image = slope * image.astype(np.float64)\n",
    "                image = image.astype(np.int16)\n",
    "\n",
    "            \n",
    "            image += np.int16(intercept)        \n",
    "\n",
    "            image = np.stack([image, image, image])\n",
    "            image = image.astype('float32')\n",
    "            image = image - image.min()\n",
    "            image = image / image.max()\n",
    "            image = image * 255.0\n",
    "            image = image.transpose(1,2,0)\n",
    "\n",
    "            if records.loc[0, \"class_id\"] == 0:\n",
    "                records = records.loc[[0], :]\n",
    "\n",
    "            boxes = records[['x_min', 'y_min', 'x_max', 'y_max']].values\n",
    "            boxes = torch.tensor(boxes, dtype = torch.float32)\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            area = torch.as_tensor(area, dtype=torch.float32)\n",
    "            labels = torch.tensor(records[\"class_id\"].values, dtype=torch.int64)\n",
    "\n",
    "            # suppose all instances are not crowd\n",
    "            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target['boxes'] = boxes\n",
    "            target['labels'] = labels\n",
    "            target['id'] = torch.tensor([index])\n",
    "            target['area'] = area\n",
    "            target['iscrowd'] = iscrowd\n",
    "\n",
    "\n",
    "            if self.transforms:\n",
    "                sample = {\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                }\n",
    "                sample = self.transforms(**sample)\n",
    "                image = sample['image']\n",
    "                target['boxes'] = torch.tensor(sample['bboxes'])\n",
    "                target['boxes'] = torch.as_tensor(target['boxes'], dtype = torch.float32)\n",
    "\n",
    "            if target[\"boxes\"].shape[0] == 0:\n",
    "                # Albumentation cuts the target (class 14, 1x1px in the corner)\n",
    "                target[\"boxes\"] = torch.from_numpy(np.array([[0.0, 0.0, 1.0, 1.0]]))\n",
    "                target['boxes'] = torch.as_tensor(target['boxes'], dtype = torch.float32)\n",
    "                target[\"area\"] = torch.tensor([1.0], dtype=torch.float32)\n",
    "                target[\"labels\"] = torch.tensor([0], dtype=torch.int64)\n",
    "            \n",
    "            return image, target, image_ids\n",
    "        \n",
    "        else:\n",
    "                   \n",
    "            image_id = self.image_ids[index]\n",
    "            records = self.df[(self.df['id'] == image_id)]\n",
    "            records = records.reset_index(drop=True)\n",
    "\n",
    "            image = dicom2array(self.df[(self.df['id'] == image_id)]['image_path'].values[0])#dcmread\n",
    "\n",
    "            #image = ds.pixel_array\n",
    "\n",
    "            intercept =  0.0\n",
    "            slope = 1.0\n",
    "\n",
    "            if slope != 1:\n",
    "                image = slope * image.astype(np.float32)\n",
    "                image = image.astype(np.int16)\n",
    "\n",
    "            image += np.int16(intercept)\n",
    "\n",
    "\n",
    "            image = np.stack([image, image, image])\n",
    "            image = image.astype('float32')\n",
    "            image = image - image.min()\n",
    "            image = image / image.max()\n",
    "            image = image * 255.0\n",
    "            image = image.transpose(1,2,0)\n",
    "\n",
    "            if self.transforms:\n",
    "                sample = {\n",
    "                    'image': image,\n",
    "                }\n",
    "                sample = self.transforms(**sample)\n",
    "                image = sample['image']\n",
    "\n",
    "            return image, image_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "# Albumentations\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=45, p=0.25),\n",
    "        A.LongestMaxSize(max_size=800, p=1.0),\n",
    "        A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_test_transform():\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ])\n",
    "\n",
    "# Thanks https://www.kaggle.com/pestipeti/competition-metric-details-script\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_iou(gt, pr, form='pascal_voc') -> float:\n",
    "    \"\"\"Calculates the Intersection over Union.\n",
    "\n",
    "    Args:\n",
    "        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n",
    "        pr: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n",
    "        form: (str) gt/pred coordinates format\n",
    "            - pascal_voc: [xmin, ymin, xmax, ymax]\n",
    "            - coco: [xmin, ymin, w, h]\n",
    "    Returns:\n",
    "        (float) Intersection over union (0.0 <= iou <= 1.0)\n",
    "    \"\"\"\n",
    "    if form == 'coco':\n",
    "        gt = gt.copy()\n",
    "        pr = pr.copy()\n",
    "\n",
    "        gt[2] = gt[0] + gt[2]\n",
    "        gt[3] = gt[1] + gt[3]\n",
    "        pr[2] = pr[0] + pr[2]\n",
    "        pr[3] = pr[1] + pr[3]\n",
    "\n",
    "    # Calculate overlap area\n",
    "    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n",
    "    \n",
    "    if dx < 0:\n",
    "        return 0.0\n",
    "    \n",
    "    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n",
    "\n",
    "    if dy < 0:\n",
    "        return 0.0\n",
    "\n",
    "    overlap_area = dx * dy\n",
    "\n",
    "\n",
    "    # Calculate overlap area\n",
    "    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n",
    "    \n",
    "    if dx < 0:\n",
    "        return 0.0\n",
    "    \n",
    "    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n",
    "\n",
    "    if dy < 0:\n",
    "        return 0.0\n",
    "\n",
    "    overlap_area = dx * dy\n",
    "\n",
    "    # Calculate union area\n",
    "    union_area = (\n",
    "            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n",
    "            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n",
    "            overlap_area\n",
    "    )\n",
    "\n",
    "    return overlap_area / union_area\n",
    "\n",
    "@jit(nopython=True)\n",
    "def find_best_match(gts, pred, pred_idx, threshold = 0.5, form = 'pascal_voc', ious=None) -> int:\n",
    "    \"\"\"Returns the index of the 'best match' between the\n",
    "    ground-truth boxes and the prediction. The 'best match'\n",
    "    is the highest IoU. (0.0 IoUs are ignored).\n",
    "\n",
    "    Args:\n",
    "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
    "        pred: (List[Union[int, float]]) Coordinates of the predicted box\n",
    "        pred_idx: (int) Index of the current predicted box\n",
    "        threshold: (float) Threshold\n",
    "        form: (str) Format of the coordinates\n",
    "        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n",
    "\n",
    "    Return:\n",
    "        (int) Index of the best match GT box (-1 if no match above threshold)\n",
    "    \"\"\"\n",
    "    best_match_iou = -np.inf\n",
    "    best_match_idx = -1\n",
    "\n",
    "\n",
    "    for gt_idx in range(len(gts)):\n",
    "        \n",
    "        if gts[gt_idx][0] < 0:\n",
    "            # Already matched GT-box\n",
    "            continue\n",
    "        \n",
    "        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n",
    "\n",
    "        if iou < 0:\n",
    "            iou = calculate_iou(gts[gt_idx], pred, form=form)\n",
    "            \n",
    "            if ious is not None:\n",
    "                ious[gt_idx][pred_idx] = iou\n",
    "\n",
    "        if iou < threshold:\n",
    "            continue\n",
    "\n",
    "        if iou > best_match_iou:\n",
    "            best_match_iou = iou\n",
    "            best_match_idx = gt_idx\n",
    "\n",
    "    return best_match_idx\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_precision(gts, preds, threshold = 0.5, form = 'coco', ious=None) -> float:\n",
    "    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n",
    "\n",
    "    Args:\n",
    "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
    "        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n",
    "               sorted by confidence value (descending)\n",
    "        threshold: (float) Threshold\n",
    "        form: (str) Format of the coordinates\n",
    "        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n",
    "\n",
    "    Return:\n",
    "        (float) Precision\n",
    "    \"\"\"\n",
    "    n = len(preds)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    # for pred_idx, pred in enumerate(preds_sorted):\n",
    "    for pred_idx in range(n):\n",
    "\n",
    "        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx,\n",
    "                                            threshold=threshold, form=form, ious=ious)\n",
    "\n",
    "        if best_match_gt_idx >= 0:\n",
    "            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n",
    "            tp += 1\n",
    "            # Remove the matched GT box\n",
    "            gts[best_match_gt_idx] = -1\n",
    "\n",
    "        else:\n",
    "            # No match\n",
    "            # False positive: indicates a predicted box had no associated gt box.\n",
    "            fp += 1\n",
    "\n",
    "    # False negative: indicates a gt box had no associated predicted box.\n",
    "    fn = (gts.sum(axis=1) > 0).sum()\n",
    "\n",
    "    return tp / (tp + fp + fn)\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_image_precision(gts, preds, thresholds = (0.5, ), form = 'coco') -> float:\n",
    "    \"\"\"Calculates image precision.\n",
    "       The mean average precision at different intersection over union (IoU) thresholds.\n",
    "\n",
    "    Args:\n",
    "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
    "        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n",
    "               sorted by confidence value (descending)\n",
    "        thresholds: (float) Different thresholds\n",
    "        form: (str) Format of the coordinates\n",
    "\n",
    "    Return:\n",
    "        (float) Precision\n",
    "    \"\"\"\n",
    "    n_threshold = len(thresholds)\n",
    "    image_precision = 0.0\n",
    "    \n",
    "    ious = np.ones((len(gts), len(preds))) * -1\n",
    "    # ious = None\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        precision_at_threshold = calculate_precision(gts.copy(), preds, threshold=threshold,\n",
    "                                                     form=form, ious=ious)\n",
    "        image_precision += precision_at_threshold / n_threshold\n",
    "\n",
    "    return image_precision\n",
    "\n",
    "iou_thresholds = [0.5]\n",
    "\n",
    "\n",
    "class EvalMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.image_precision = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, gt_boxes, pred_boxes, n=1):       \n",
    "        \"\"\" pred_boxes : need to be sorted.\"\"\"\n",
    "        \n",
    "        self.image_precision = calculate_image_precision(pred_boxes,\n",
    "                                                         gt_boxes,\n",
    "                                                         thresholds=iou_thresholds,\n",
    "                                                         form='pascal_voc')\n",
    "        self.count += n\n",
    "        self.sum += self.image_precision * n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f698a9-3712-491f-82e5-d1c462443e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRCNNPredictornew(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box regression layers\n",
    "    for Fast R-CNN.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(FastRCNNPredictornew, self).__init__()\n",
    "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
    "        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n",
    "        self.cls_list = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        self.cls_list.append(scores)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas\n",
    "\n",
    "    def print_cls_score(self):\n",
    "        print(self.cls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39273456-ee1a-4f9c-878f-2def9ad32d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #50 loss: 0.27529218792915344\n",
      "Iteration #100 loss: 0.2427007555961609\n",
      "Iteration #150 loss: 0.22743350267410278\n",
      "Iteration #200 loss: 0.25621405243873596\n",
      "Iteration #250 loss: 0.29965585470199585\n",
      "Iteration #300 loss: 0.12674552202224731\n",
      "Iteration #350 loss: 0.1669808179140091\n",
      "Iteration #400 loss: 0.2573414444923401\n",
      "Iteration #450 loss: 0.29841452836990356\n",
      "Iteration #500 loss: 0.27481332421302795\n",
      "Iteration #550 loss: 0.19184733927249908\n",
      "Iteration #600 loss: 0.2886522114276886\n",
      "Iteration #650 loss: 0.2192782312631607\n",
      "Epoch #0 loss: 0.24965485997464523\n",
      "Iteration #700 loss: 0.20118990540504456\n",
      "Iteration #750 loss: 0.13909512758255005\n",
      "Iteration #800 loss: 0.19838851690292358\n",
      "Iteration #850 loss: 0.24703572690486908\n",
      "Iteration #900 loss: 0.22190500795841217\n",
      "Iteration #950 loss: 0.30124539136886597\n",
      "Iteration #1000 loss: 0.29644155502319336\n",
      "Iteration #1050 loss: 0.19309909641742706\n",
      "Iteration #1100 loss: 0.17005914449691772\n",
      "Iteration #1150 loss: 0.18867462873458862\n",
      "Iteration #1200 loss: 0.19236138463020325\n",
      "Iteration #1250 loss: 0.1778588593006134\n",
      "Iteration #1300 loss: 0.11022256314754486\n",
      "Epoch #1 loss: 0.2212873932303696\n",
      "Iteration #1350 loss: 0.19823801517486572\n",
      "Iteration #1400 loss: 0.135520339012146\n",
      "Iteration #1450 loss: 0.17522528767585754\n",
      "Iteration #1500 loss: 0.2658160328865051\n",
      "Iteration #1550 loss: 0.2633313536643982\n",
      "Iteration #1600 loss: 0.17075252532958984\n",
      "Iteration #1650 loss: 0.21260420978069305\n",
      "Iteration #1700 loss: 0.3210456371307373\n",
      "Iteration #1750 loss: 0.21565528213977814\n",
      "Iteration #1800 loss: 0.2097521424293518\n",
      "Iteration #1850 loss: 0.21281056106090546\n",
      "Iteration #1900 loss: 0.12339963018894196\n",
      "Iteration #1950 loss: 0.20158763229846954\n",
      "Iteration #2000 loss: 0.2523689568042755\n",
      "Epoch #2 loss: 0.21078559101141733\n",
      "Iteration #2050 loss: 0.24026581645011902\n",
      "Iteration #2100 loss: 0.2692912220954895\n",
      "Iteration #2150 loss: 0.22671698033809662\n",
      "Iteration #2200 loss: 0.250778466463089\n",
      "Iteration #2250 loss: 0.19443465769290924\n",
      "Iteration #2300 loss: 0.15550363063812256\n",
      "Iteration #2350 loss: 0.17881786823272705\n",
      "Iteration #2400 loss: 0.15861645340919495\n",
      "Iteration #2450 loss: 0.18557427823543549\n",
      "Iteration #2500 loss: 0.2620892822742462\n",
      "Iteration #2550 loss: 0.19607701897621155\n",
      "Iteration #2600 loss: 0.2172192931175232\n",
      "Iteration #2650 loss: 0.15739642083644867\n",
      "Epoch #3 loss: 0.20487596708974024\n",
      "Iteration #2700 loss: 0.12358194589614868\n",
      "Iteration #2750 loss: 0.15020649135112762\n",
      "Iteration #2800 loss: 0.07061231136322021\n",
      "Iteration #2850 loss: 0.18376915156841278\n",
      "Iteration #2900 loss: 0.179253488779068\n",
      "Iteration #2950 loss: 0.10375910252332687\n",
      "Iteration #3000 loss: 0.1059323251247406\n",
      "Iteration #3050 loss: 0.050008755177259445\n",
      "Iteration #3100 loss: 0.16201268136501312\n",
      "Iteration #3150 loss: 0.23548217117786407\n",
      "Iteration #3200 loss: 0.12488953769207001\n",
      "Iteration #3250 loss: 0.13704289495944977\n",
      "Iteration #3300 loss: 0.19824793934822083\n",
      "Epoch #4 loss: 0.16343021301784436\n",
      "Iteration #3350 loss: 0.11075325310230255\n",
      "Iteration #3400 loss: 0.1686539351940155\n",
      "Iteration #3450 loss: 0.13497722148895264\n",
      "Iteration #3500 loss: 0.11683765053749084\n",
      "Iteration #3550 loss: 0.14775583148002625\n",
      "Iteration #3600 loss: 0.1750795990228653\n",
      "Iteration #3650 loss: 0.17738942801952362\n",
      "Iteration #3700 loss: 0.12568068504333496\n",
      "Iteration #3750 loss: 0.09104150533676147\n",
      "Iteration #3800 loss: 0.2699604332447052\n",
      "Iteration #3850 loss: 0.17076551914215088\n",
      "Iteration #3900 loss: 0.16203325986862183\n",
      "Iteration #3950 loss: 0.20434923470020294\n",
      "Iteration #4000 loss: 0.09550401568412781\n",
      "Epoch #5 loss: 0.15776949330907444\n",
      "Iteration #4050 loss: 0.1757780760526657\n",
      "Iteration #4100 loss: 0.17245414853096008\n",
      "Iteration #4150 loss: 0.19889262318611145\n",
      "Iteration #4200 loss: 0.15371939539909363\n",
      "Iteration #4250 loss: 0.1740880310535431\n",
      "Iteration #4300 loss: 0.17467668652534485\n",
      "Iteration #4350 loss: 0.2655925154685974\n",
      "Iteration #4400 loss: 0.1428135633468628\n",
      "Iteration #4450 loss: 0.20274505019187927\n",
      "Iteration #4500 loss: 0.1931600719690323\n",
      "Iteration #4550 loss: 0.14410316944122314\n",
      "Iteration #4600 loss: 0.11911184340715408\n",
      "Iteration #4650 loss: 0.13853752613067627\n",
      "Epoch #6 loss: 0.16242750017796231\n",
      "Iteration #4700 loss: 0.14096325635910034\n",
      "Iteration #4750 loss: 0.168426513671875\n",
      "Iteration #4800 loss: 0.20375947654247284\n",
      "Iteration #4850 loss: 0.18301552534103394\n",
      "Iteration #4900 loss: 0.13316692411899567\n",
      "Iteration #4950 loss: 0.6186400055885315\n",
      "Iteration #5000 loss: 0.03498811274766922\n",
      "Iteration #5050 loss: 0.15562888979911804\n",
      "Iteration #5100 loss: 0.1291206181049347\n",
      "Iteration #5150 loss: 0.17726252973079681\n",
      "Iteration #5200 loss: 0.14561119675636292\n",
      "Iteration #5250 loss: 0.13565142452716827\n",
      "Iteration #5300 loss: 0.201711505651474\n",
      "Epoch #7 loss: 0.15264093556414837\n",
      "Iteration #5350 loss: 0.08178767561912537\n",
      "Iteration #5400 loss: 0.11437466740608215\n",
      "Iteration #5450 loss: 0.09638315439224243\n",
      "Iteration #5500 loss: 0.1480979025363922\n",
      "Iteration #5550 loss: 0.17436698079109192\n",
      "Iteration #5600 loss: 0.11031447350978851\n",
      "Iteration #5650 loss: 0.2737455368041992\n",
      "Iteration #5700 loss: 0.11988521367311478\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictornew(in_features, 2)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = COVIDTrainDataLoader(train_df, get_train_transform())\n",
    "valid_dataset = COVIDTrainDataLoader(valid_df, get_valid_transform())\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "# Create train and validate data loader\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,   \n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "num_epochs =  10 #low epoch to save GPU time\n",
    "\n",
    "loss_hist = Averager()\n",
    "itr = 1\n",
    "lossHistoryiter = []\n",
    "lossHistoryepoch = []\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        loss_dict = model(images, targets) \n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        loss_hist.send(loss_value)\n",
    "        lossHistoryiter.append(loss_value)\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "    lossHistoryepoch.append(loss_hist.value)\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")   \n",
    "    \n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time taken to Train the model :{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e515714b-c9f3-4a2f-838c-cfac1042d009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  1 19:50:17 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Quadro R...  Off  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   38C    P0    65W / 250W |  22693MiB / 22698MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3313      C   ...nvs/tensorflow/bin/python    12063MiB |\n",
      "|    0   N/A  N/A     10710      C   ...vs/yourevnname/bin/python      267MiB |\n",
      "|    0   N/A  N/A     12561      C   .../envs/yourname/bin/python    10123MiB |\n",
      "|    0   N/A  N/A     29867      C   ...nvs/tensorflow/bin/python      237MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d2742b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class COVIDValidDataLoader(Dataset): #Class to load Training Data\n",
    "    \n",
    "    def __init__(self, dataframe, transforms=None,stat = 'Train'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_ids = dataframe[\"id\"].unique()\n",
    "        \n",
    "        self.df = dataframe\n",
    "        self.transforms = transforms\n",
    "        self.stat = stat\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.stat == 'Train':\n",
    "            \n",
    "            image_id = self.image_ids[index]\n",
    "            \n",
    "            records = self.df[(self.df['id'] == image_id)]\n",
    "            records = records.reset_index(drop=True)\n",
    "            image = dicom2array(self.df[(self.df['id'] == image_id)]['image_path'].values[0])#dcmread\n",
    "\n",
    "            #image = ds.pixel_array\n",
    "           \n",
    "            '''if \"PhotometricInterpretation\" in dicom:\n",
    "                if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "                    image = np.amax(image) - image'''\n",
    "\n",
    "            intercept =  0.0\n",
    "            slope =1.0\n",
    "\n",
    "            if slope != 1:\n",
    "                image = slope * image.astype(np.float64)\n",
    "                image = image.astype(np.int16)\n",
    "\n",
    "            \n",
    "            image += np.int16(intercept)        \n",
    "\n",
    "            image = np.stack([image, image, image])\n",
    "            image = image.astype('float32')\n",
    "            image = image - image.min()\n",
    "            image = image / image.max()\n",
    "            image = image * 255.0\n",
    "            image = image.transpose(1,2,0)\n",
    "\n",
    "            if records.loc[0, \"class_id\"] == 0:\n",
    "                records = records.loc[[0], :]\n",
    "\n",
    "            boxes = records[['x_min', 'y_min', 'x_max', 'y_max']].values\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            area = torch.as_tensor(area, dtype=torch.float32)\n",
    "            labels = torch.tensor(records[\"class_id\"].values, dtype=torch.int64)\n",
    "\n",
    "            # suppose all instances are not crowd\n",
    "            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target['boxes'] = boxes\n",
    "            target['labels'] = labels\n",
    "            #target['id'] = torch.tensor([index])\n",
    "            target['id'] = image_id\n",
    "            target['area'] = area\n",
    "            target['iscrowd'] = iscrowd\n",
    "\n",
    "            if self.transforms:\n",
    "                sample = {\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                }\n",
    "                sample = self.transforms(**sample)\n",
    "                image = sample['image']\n",
    "\n",
    "                target['boxes'] = torch.tensor(sample['bboxes'])\n",
    "\n",
    "            if target[\"boxes\"].shape[0] == 0:\n",
    "                # Albumentation cuts the target (class 14, 1x1px in the corner)\n",
    "                target[\"boxes\"] = torch.from_numpy(np.array([[0.0, 0.0, 1.0, 1.0]]))\n",
    "                target[\"area\"] = torch.tensor([1.0], dtype=torch.float32)\n",
    "                target[\"labels\"] = torch.tensor([0], dtype=torch.int64)\n",
    "            \n",
    "            return image, target, image_id\n",
    "        \n",
    "        else:\n",
    "                   \n",
    "            image_id = self.image_ids[index]\n",
    "            records = self.df[(self.df['id'] == image_id)]\n",
    "            records = records.reset_index(drop=True)\n",
    "\n",
    "            image = dicom2array(self.df[(self.df['id'] == image_id)]['image_path'].values[0])#dcmread\n",
    "\n",
    "            #image = ds.pixel_array\n",
    "\n",
    "            intercept =  0.0\n",
    "            slope = 1.0\n",
    "\n",
    "            if slope != 1:\n",
    "                image = slope * image.astype(np.float64)\n",
    "                image = image.astype(np.int16)\n",
    "\n",
    "            image += np.int16(intercept)        \n",
    "\n",
    "            image = np.stack([image, image, image])\n",
    "            image = image.astype('float32')\n",
    "            image = image - image.min()\n",
    "            image = image / image.max()\n",
    "            image = image * 255.0\n",
    "            image = image.transpose(1,2,0)\n",
    "\n",
    "            if self.transforms:\n",
    "                sample = {\n",
    "                    'image': image,\n",
    "                }\n",
    "                sample = self.transforms(**sample)\n",
    "                image = sample['image']\n",
    "\n",
    "            return image, image_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.image_ids.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be8c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = COVIDValidDataLoader(valid_df, get_valid_transform())\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8773307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction_string(image_id,labels, boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(labels, scores, boxes):\n",
    "#         pred_strings.append(\"{0} {1:.4f} {2} {3} {4} {5}\".format(\n",
    "#             j[0], j[1], j[2][0], j[2][1], j[2][2], j[2][3]))\n",
    "          pred_strings.append([image_id,j[0], j[1], j[2][0], j[2][1], j[2][2], j[2][3]])\n",
    "\n",
    "    return pred_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e354faa2-db3d-4ccc-b6ea-5d77c4118603",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 22.17 GiB total capacity; 2.39 GiB already allocated; 5.81 MiB free; 8.74 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f659dc49e71a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yourname/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yourname/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yourname/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yourname/lib/python3.8/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yourname/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yourname/lib/python3.8/site-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0manchors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yourname/lib/python3.8/site-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mgrid_anchors\u001b[0;34m(self, grid_sizes, strides)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mshift_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mshift_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mshifts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# For every (base anchor, output anchor) pair,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 22.17 GiB total capacity; 2.39 GiB already allocated; 5.81 MiB free; 8.74 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "detection_threshold = 0.5\n",
    "results = []\n",
    "answers = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pred_boxes (list) : [[train_idx, class_pred, prob_score, x1, y1, x2, y2], ... ]    \n",
    "    for images, targets, image_ids in valid_data_loader:\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        for i,target in enumerate(targets):\n",
    "            answer = [target['id'],target['labels'][0].item(),1, \n",
    "                      target['boxes'][0][0].item(), target['boxes'][0][1].item(), target['boxes'][0][2].item(), target['boxes'][0][3].item()]\n",
    "            answers.append(answer)\n",
    "        for i, image in enumerate(images):\n",
    "            \n",
    "            image_id = image_ids[i]\n",
    "\n",
    "            result = {\n",
    "                'image_id': image_id,\n",
    "                'PredictionString': '14 1.0 0 0 1 1'\n",
    "            }\n",
    "\n",
    "            boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "            labels = outputs[i]['labels'].data.cpu().numpy()\n",
    "            scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "\n",
    "            if len(boxes) > 0:\n",
    "\n",
    "#                 labels = labels - 1\n",
    "#                 labels[labels == -1] = 1\n",
    "\n",
    "                selected = scores >= detection_threshold\n",
    "\n",
    "                boxes = boxes[selected].astype(np.int32)\n",
    "                scores = scores[selected]\n",
    "                labels = labels[selected]\n",
    "\n",
    "                if len(boxes) > 0:\n",
    "#                     result = {\n",
    "#                         'image_id': image_id,\n",
    "#                         'PredictionString': format_prediction_string(image_id,labels, boxes, scores)\n",
    "#                     }\n",
    "                    \n",
    "                    result = format_prediction_string(image_id,labels,boxes,scores)\n",
    "\n",
    "            results.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f33e5-56b5-4ed3-b823-8c83ccb2d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3ac8e-d5bf-47be-8dd6-9d9b2a86f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "def iou_calc(bb1, bb2):\n",
    "    ## 오른쪽 좌표가 왼쪽 좌표보다 커야 하고, 위 좌표가 아래 좌표보다 커야 함 그렇지 않을 경우 asserterror\n",
    "#     assert bb1['x1'] < bb1['x2']\n",
    "#     assert bb1['y1'] < bb1['y2']\n",
    "#     assert bb2['x1'] < bb2['x2']\n",
    "#     assert bb2['y1'] < bb2['y2']\n",
    "\n",
    "    ## 두개의 bounding box가 겹치는 영역의 좌표\n",
    "    x_left = max(bb1[0], bb2[0])\n",
    "    x_right = min(bb1[2], bb2[2])\n",
    "    y_bottom = max(bb1[1], bb2[1])\n",
    "    y_top = min(bb1[3], bb2[3])\n",
    "\n",
    "    if x_right < x_left or y_top < y_bottom: return 0\n",
    "\n",
    "    intersection_area = (x_right - x_left) * (y_top - y_bottom)\n",
    "\n",
    "    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "\n",
    "    iou = intersection_area / (bb1_area + bb2_area - intersection_area)\n",
    "\n",
    "    assert iou <= 1\n",
    "    assert iou >= 0\n",
    "    return iou\n",
    "\n",
    "def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format='corners', num_classes=2):\n",
    "    # pred_boxes (list) : [[train_idx, class_pred, prob_score, x1, y1, x2, y2], ... ]\n",
    "    average_precisions = []\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    # 각각의 클래스에 대한 AP를 구합니다.\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # 모델이 c를 검출한 bounding box를 detections에 추가합니다.\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        # 실제 c 인 bounding box를 ground_truths에 추가합니다.\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # amount_bboxes에 class에 대한 bounding box 개수를 저장합니다.\n",
    "        # 예를 들어, img 0은 3개의 bboxes를 갖고 있고 img 1은 5개의 bboxes를 갖고 있으면\n",
    "        # amount_bboexs = {0:3, 1:5} 가 됩니다.\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # class에 대한 bounding box 개수 만큼 0을 추가합니다.\n",
    "        # amount_boxes = {0:torch.tensor([0,0,0]), 1:torch.tensor([0,0,0,0,0])}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # detections를 정확도 높은 순으로 정렬합니다.\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "\t\t# TP와 FP를 구합니다.\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = iou_calc(torch.tensor(detection[3:]),\n",
    "                                              torch.tensor(gt[3:]),)\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "        \n",
    "        # cumsum은 누적합을 의미합니다.\n",
    "        # [1, 1, 0, 1, 0] -> [1, 2, 2, 3, 3]\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]),recalls))\n",
    "        \n",
    "        # torch.trapz(y,x) : x-y 그래프를 적분합니다.\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9206631-c45e-4b9b-8f91-0c9a5cfb15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_average_precision(results,answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32fe1db-8fce-40e0-bf80-2a3477f864c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # pred_boxes (list) : [[train_idx, class_pred, prob_score, x1, y1, x2, y2], ... ]    \n",
    "    for images, targets, image_ids in valid_data_loader:\n",
    "\n",
    "#이런거 있는 코드 돌리시면 valid 데이터셋의 예측 결과가 정리된 results가 만들어져요. 일단 results 있으면 시각화 코드는 돌릴 수 있습니다. 그치만 순서 헷갈리니까 mean_average_precision(results,answers) 코드 다음에 붙여서 쓰시는것을 추천 드립니다..\n",
    "\n",
    "#4개 클래스 구분할 경우 시각화:\n",
    "#1) 필요한 데이터와 함수 정의\n",
    "results_df=pd.DataFrame(results)\n",
    "results_df.columns=['image_id', 'pred_type', 'prob_score', 'x_min', 'y_min', 'x_max', 'y_max']\n",
    "\n",
    "imgs = []\n",
    "valid_image_ids = results_df['image_id'].values\n",
    "type_ids = train_image_df['type_id']\n",
    "\n",
    "# map label_id to specify color\n",
    "#label2color = {type_id:[random.randint(0,255) for i in range(3)] for type_id in type_ids}\n",
    "label2color = {0: [0, 0, 0],\n",
    "               1: [255, 0, 0],\n",
    "               2: [0, 255, 0],\n",
    "               3: [0, 0, 255]}\n",
    "thickness = 3\n",
    "scale = 5\n",
    "\n",
    "def label_to_name(label):\n",
    "    if label == 0 : name =\"Negative for Pneumonia\"\n",
    "    elif label == 1 : name = \"Typical Appearance\"\n",
    "    elif label == 2: name = \"Indeterminate Appearance\"\n",
    "    elif label == 3: name = \"Atypical Appearance\"\n",
    "    return name\n",
    "\n",
    "\n",
    "#2) ground truth 박스 그리기\n",
    "sample_paths=[]\n",
    "for i in range(8):\n",
    "    id = random.choice(valid_image_paths)\n",
    "    image_path = valid_df.loc[valid_df['id']==id, ['image_path']].values[0][0]\n",
    "    img = dicom2array(str(image_path))\n",
    "    img = cv2.resize(img, None, fx=1/scale, fy=1/scale)\n",
    "    img = np.stack([img, img, img], axis=-1)\n",
    "    label = valid_df.loc[valid_df['image_path'] == image_path, ['type_id']].values[0][0]\n",
    "    \n",
    "    sample_paths.append(image_path)\n",
    "    print(image_path)\n",
    "    \n",
    "    boxes=[]\n",
    "    num = valid_df.loc[valid_df['id']==id, ['image_path']].count()[0]\n",
    "    for j in range(num):\n",
    "        box = valid_df.loc[valid_df['image_path'] == image_path, ['x_min', 'y_min', 'x_max', 'y_max']].values[j]/scale\n",
    "        boxes.append(box)\n",
    "        \n",
    "        color = label2color[label]\n",
    "        img = cv2.rectangle(\n",
    "            img,\n",
    "            (int(boxes[j][0]), int(boxes[j][1])),\n",
    "            (int(boxes[j][2]), int(boxes[j][3])),\n",
    "            color, thickness)\n",
    "        img = cv2.putText(img, label_to_name(label), (int(boxes[j][0]), int(boxes[j][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3, cv2.LINE_AA)\n",
    "\n",
    "    img = cv2.resize(img, (500,500))\n",
    "    imgs.append(img)\n",
    "    print(boxes)\n",
    "    \n",
    "    plot_img(img, cmap=None)\n",
    "\n",
    "\n",
    "    #3) predict 박스 그리기\n",
    "for i in range(8):\n",
    "    image_path = sample_paths[i]\n",
    "    image_id = sample_ids[i]\n",
    "    print(sample_paths[i])\n",
    "    \n",
    "    img = dicom2array(str(image_path))\n",
    "    img = cv2.resize(img, None, fx=1/scale, fy=1/scale)\n",
    "    img = np.stack([img, img, img], axis=-1)\n",
    "    label = results_df.loc[results_df['image_id'] == image_id, ['pred_type']].values[0][0]\n",
    "    \n",
    "    boxes=[]\n",
    "    num = results_df.loc[results_df['image_id'] == image_id].count()[0]\n",
    "    for j in range(num):\n",
    "        box = results_df.loc[results_df['image_id'] == image_id, ['x_min', 'y_min', 'x_max', 'y_max']].values[j]/scale\n",
    "        boxes.append(box)\n",
    "        \n",
    "        color = label2color[label]\n",
    "        img = cv2.rectangle(\n",
    "            img,\n",
    "            (int(boxes[j][0]), int(boxes[j][1])),\n",
    "            (int(boxes[j][2]), int(boxes[j][3])),\n",
    "            color, thickness)\n",
    "        img = cv2.putText(img, label_to_name(label), (int(boxes[j][0]), int(boxes[j][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3, cv2.LINE_AA)\n",
    "    \n",
    "    img = cv2.resize(img, (500,500))\n",
    "    imgs.append(img)\n",
    "    print(boxes)\n",
    "    \n",
    "    plot_img(img, cmap=None)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
